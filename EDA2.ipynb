{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from catboost import Pool,CatBoostClassifier\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold , KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import lightgbm as lgbm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List ,Dict, Tuple\n",
    "from sklearn.metrics import log_loss\n",
    "import lightgbm as lgbm\n",
    "from optuna import trial\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "import seaborn as sns\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from matplotlib import pyplot\n",
    "from optuna.trial import Trial\n",
    "DATA_PATH = './JobCare_data/'\n",
    "train = pd.read_csv('./JobCare_data/train.csv')\n",
    "code_d = pd.read_csv(f'{DATA_PATH}속성_D_코드.csv').iloc[:,:-1]\n",
    "code_h = pd.read_csv(f'{DATA_PATH}속성_H_코드.csv')\n",
    "code_l = pd.read_csv(f'{DATA_PATH}속성_L_코드.csv')\n",
    "train_data = pd.read_csv(f'{DATA_PATH}train.csv')\n",
    "test_data = pd.read_csv(f'{DATA_PATH}test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "nominal_cols = ['person_attribute_a_1','person_attribute_b','person_prefer_e','contents_attribute_e']\n",
    "code_d.columns  = [\"attribute_d\", \"attribute_d_d\" , \"attribute_d_s\", \"attribute_d_m\" , \"attribute_d_l\"]\n",
    "code_h.columns = [\"attribute_h\" , \"attribute_h_p\"]\n",
    "code_l.columns = [\"attribute_l\", \"attribute_l_d\", \"attribute_l_s\" , \"attribute_l_m\", \"attribute_l_l\"]\n",
    "ordinal_cols = ['person_attribute_a_1','person_attribute_b', 'person_prefer_e', 'contents_attribute_e']\n",
    "# 오버피팅이 자주 발생하는 mean encoding 이므로 cross validation 과 정규화 같이 사용한다=\n",
    "# 변환 하고자 하는 범주형 변수 선택\n",
    "# 범주형 변수 그룹화 -> 타깃 변수 총합 합계\n",
    "# 범주형 변수 그룹화 타깃 빈도수 합계\n",
    "# 총합을 카운트로 나누고 본래 범주 값에 업데이트\n",
    "# 여러가지 방법으로 적용 가능하다\n",
    "# 비슷한 범주 사이에 있는 관계 표현 특징, 범주와 타깃사이에만 국한된다\n",
    "# 범주가 많은 경우 이 방법은 데이터를 훨씬 더 단순화 한다\n",
    " #%%\n",
    "cols_merge = [\n",
    "              (\"person_prefer_d_1\" , code_d),\n",
    "              (\"person_prefer_d_2\" , code_d),\n",
    "              (\"person_prefer_d_3\" , code_d),\n",
    "              (\"contents_attribute_d\" , code_d),\n",
    "              (\"person_prefer_h_1\" , code_h),\n",
    "              (\"person_prefer_h_2\" , code_h),\n",
    "              (\"person_prefer_h_3\" , code_h),\n",
    "              (\"contents_attribute_h\" , code_h),\n",
    "              (\"contents_attribute_l\" , code_l),\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 회원 속성과 콘텐츠 속성의 동일한 코드 여부에 대한 컬럼명 리스트\n",
    "cols_equi = [\n",
    "    (\"contents_attribute_c\",\"person_prefer_c\"),\n",
    "    (\"contents_attribute_e\",\"person_prefer_e\"),\n",
    "    (\"person_prefer_d_2_attribute_d_s\" , \"contents_attribute_d_attribute_d_s\"),\n",
    "    (\"person_prefer_d_2_attribute_d_m\" , \"contents_attribute_d_attribute_d_m\"),\n",
    "    (\"person_prefer_d_2_attribute_d_l\" , \"contents_attribute_d_attribute_d_l\"),\n",
    "    (\"person_prefer_d_3_attribute_d_s\" , \"contents_attribute_d_attribute_d_s\"),\n",
    "    (\"person_prefer_d_3_attribute_d_m\" , \"contents_attribute_d_attribute_d_m\"),\n",
    "    (\"person_prefer_d_3_attribute_d_l\" , \"contents_attribute_d_attribute_d_l\"),\n",
    "    (\"person_prefer_h_1_attribute_h_p\" , \"contents_attribute_h_attribute_h_p\"),\n",
    "    (\"person_prefer_h_2_attribute_h_p\" , \"contents_attribute_h_attribute_h_p\"),\n",
    "    (\"person_prefer_h_3_attribute_h_p\" , \"contents_attribute_h_attribute_h_p\"),\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "train_data.drop(['id','contents_open_dt','contents_rn','person_rn'],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def merge_codes(df:pd.DataFrame, df_code:pd.DataFrame, col:str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df_code = df_code.copy()\n",
    "    df_code = df_code.add_prefix(f\"{col}_\")\n",
    "    # join key 설정\n",
    "    df_code.columns.values[0] = col\n",
    "    print('df :' ,df)\n",
    "    print('df_code : ', df_code)\n",
    "    print('merge data : ',pd.merge(df,df_code , how='left', on= col) )\n",
    "    return pd.merge(df,df_code , how='left', on= col)\n",
    "\n",
    "def preprocess_data(\n",
    "                    df:pd.DataFrame,is_train:bool = True,\n",
    "                    cols_merge:List[Tuple[str,pd.DataFrame]] = []  ,\n",
    "                    cols_equi:List[Tuple[str,str]]= [] ,\n",
    "                    cols_drop:List[str] = [\"id\",\"person_prefer_f\",\"person_prefer_g\" ,\"contents_open_dt\"]\n",
    "                    )->Tuple[pd.DataFrame,np.ndarray]:\n",
    "\n",
    "    y_data = None\n",
    "\n",
    "    if is_train:\n",
    "        y_data = df[\"target\"].to_numpy()\n",
    "        df = df.drop(columns=\"target\")\n",
    "\n",
    "    for col , df_code in cols_merge:\n",
    "        df = merge_codes(df, df_code, col)\n",
    "\n",
    "    cols = df.select_dtypes(bool).columns.tolist()\n",
    "    df[cols] = df[cols].astype(int)\n",
    "\n",
    "    for col1, col2 in cols_equi:\n",
    "        df[f\"{col1}_{col2}\"] = (df[col1] == df[col2]).astype(int)\n",
    "    df = df.drop(columns=cols_drop)\n",
    "\n",
    "    return (df,y_data)\n",
    "\n",
    "\n",
    "def get_meta_data(train_data):\n",
    "    cols = train_data.columns\n",
    "    temp =[]\n",
    "    binary_col = []\n",
    "    nominal_col = []\n",
    "    ordinal_col = []\n",
    "    for col in cols:\n",
    "        if 'yn' in col:\n",
    "            temp.append((col,'binary'))\n",
    "        elif col in ordinal_cols:\n",
    "            temp.append((col,'ordinal'))\n",
    "        else:\n",
    "            temp.append((col,'nominal'))\n",
    "            \n",
    "    temp = pd.DataFrame(data)\n",
    "    \n",
    "    temp.columns = ['col_name', 'type']\n",
    "    return temp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_mean_encode(train, nominal_cols):\n",
    "    for col in nominal_cols:\n",
    "        if 'attribute' in col:\n",
    "            name = col[col.index('attribute') + len('attribute')+1:]\n",
    "            var_name = 'attr_{}_mean_encode'.format(name)\n",
    "            locals()[var_name] = train.groupby(col)[\"target\"].mean()\n",
    "        elif 'prefer' in col:\n",
    "            name = col[col.index('prefer') + len('prefer')+1:]\n",
    "            var_name = 'prefer_{}_mean_encode'.format(name)\n",
    "            locals()[var_name] = train.groupby(col)[\"target\"].mean()\n",
    "        train.loc[:,var_name] = train[col].map(locals()[var_name])\n",
    "    return train\n",
    "\n",
    "\n",
    "def objective(trial,X,y):\n",
    "    param_grid = {\n",
    "        # \"device_type\" : trial.suggest_categorical(\"device_type\",['gpu']),\n",
    "        \"learning_rate\" : trial.suggest_float(\"learning_rate\", 0.1, 0.95, step=0.1 ),\n",
    "        \"num_leaves\" : trial.suggest_int(\"num_leaves\", 5, 100 ,step=5),\n",
    "        \"max_depth\" : trial.suggest_int(\"max_depth\", 1,30 , step=3),\n",
    "        \"min_data_in_leaf\" : trial.suggest_int(\"min_data_in_leaf\" ,50 , 1000 , step=50),\n",
    "        \"lambda_l1\" : trial.suggest_int(\"lambda_l1\", 0,100 , step=5),\n",
    "        \"lambda_l2\" : trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n",
    "        \"min_gain_to_split\" : trial.suggest_float(\"min_datin_to_split\" , 1,30, step=3),\n",
    "        \"bagging_fraction\" : trial.suggest_float(\"bagging_fraction\" , 0.1,0.99,step=0.1),\n",
    "        \"bagging_freq\" : trial.suggest_categorical(\"bagging_freq\",[1]),\n",
    "        \"feature_fraction\" : trial.suggest_float(\"feature_fraction\", 0.1,0.99,step=0.1),\n",
    "        \"n_estimators\" : trial.suggest_int(\"n_estimators\",10,2000, step=10),\n",
    "    }\n",
    "    cv=  StratifiedKFold(n_splits=5, shuffle=False)\n",
    "    cv_scores = np.empty(5)\n",
    "\n",
    "    for idx, (train_idx, test_idx ) in enumerate(cv.split(X,y)):\n",
    "        X_train , X_test = X.iloc[train_idx] , X.iloc[test_idx]\n",
    "        y_train , y_test = y[train_idx] , y[test_idx]\n",
    "        model = lgbm.LGBMClassifier(objective=\"binary\" , **param_grid)\n",
    "\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set = [(X_test, y_test)],\n",
    "            eval_metric = \"binary_logloss\",\n",
    "            early_stopping_rounds =100,\n",
    "            callbacks= [\n",
    "                LightGBMPruningCallback(trial, \"binary_logloss\")\n",
    "            ], # Add a pruning callback\n",
    "        )\n",
    "        preds = model.predict_proba(X_test)\n",
    "        cv_scores[idx] = log_loss(y_test,preds)\n",
    "\n",
    "    #return np.mean(f1_scores)\n",
    "    return np.mean(cv_scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# :::::::::::: META DATA MAKE :::::::::::::::::::\n",
    "data = get_meta_data(train_data) \n",
    "# :::::::::::: META DATA MAKE END :::::::::::::::::::\n",
    "# get mean_encdoe train set\n",
    "train = get_mean_encode(train, nominal_cols)\n",
    "target = train.target\n",
    "# drop target , contetns_open_dt, id\n",
    "train = train.drop(['contents_open_dt','id','target'],axis=1)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train,target , test_size=0.3 , shuffle=True, stratify=target, random_state=34)\n",
    "# gradient boosting 이 randomforest 보다 좋다고 알려져있다\n",
    "rf= RandomForestClassifier(n_estimators=150 , max_depth=8 , min_samples_split=4, max_features=0.2, n_jobs=-1 ,random_state=0)\n",
    "rf.fit(x_train,y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# :::::::::::: HYPER PARAMETER OPTUNA START  LGBM CLASSIFIER:::::::::::::::::::::::\n",
    "\n",
    "# mean encode값 랜포보다 성능좋다고 알려진 Lightgbm에 넣기\n",
    "study = optuna.create_study(direction=\"minimize\" , study_name = \"LGBM Classifier\")\n",
    "func = lambda trial: objective(trial,train,target )\n",
    "study.optimize(func,n_trials=20)\n",
    "Trial.suggest_float(name=\"learning_rate\",low= 0.1 ,high=0.95, step=0.1 )\n",
    "trial = Trial()\n",
    "trial.suggest_float(name=\"learning_rate\",low= 0.1 ,high=0.95, step=0.1 )\n",
    "# trial suggest_float 메서드는 self 가 필요한데 이렇게 익명함수 명을 함수명으로 해서 다른 함수의 인자로 넘기면 가능한가 ?\n",
    "train = train.drop(['person_rn','contents_rn'], axis=1)\n",
    "# 과적합일진 모르지만 그래도많이올랐따 만족할 수 없음\n",
    "# 0.57정도에서 0.59 정도로 mean encoding 으로 상승 -> 랜포말고 다른건 ?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%::::::::::::::: RADNOM FOREST END :::::::::::::::::::\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "# 시작과 끝을 잘 경계짓고 , 범주를 잘 정리 변수의 위치 등"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#define dataset\n",
    "y = y_train.values.tolist()\n",
    "X, y = make_regression(n_samples=1000, n_features=10 , n_informative=5, random_state=1)\n",
    "#evaluate the model\n",
    "model = CatBoostRegressor(verbose=0 , n_estimators=100)\n",
    "cv = RepeatedKFold(n_splits= 10 , n_repeats=3 , random_state=1)\n",
    "n_scores = cross_val_score(model, X,y , scoring='neg_mean_absolute_error', cv=cv)\n",
    "print('MAE : %.3f (%.3f)'  % (mean(n_scores), std(n_scores)))\n",
    "#fit the model on the whole dataset\n",
    "model.fit(X,y)\n",
    "# make a single prediction\n",
    "row = x_valid[[col for col in x_train.columns if 'mean_encode' in col]].values.tolist()\n",
    "#row = [[2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381, 0.87616892, -0.50446586, 0.23009474, 0.76201118]]\n",
    "# make a single prediction\n",
    "yhat = model.predict(row)\n",
    "print('Prediction: %.3f' % yhat[0])\n",
    "(yhat == y_valid.values.tolist()).mean()\n",
    "#::::::::::::: CATBOOSTREGRESSOR END :::::::::::::::::::::#\n",
    "#catboost + cv 5fold + threshold"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%::::::::::::: CATBOOSTREGRESSOR START :::::::::::::::::::::#\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#Backward difference 코딩에서\n",
    "# #수준에 대한 종속 변수의 평균은 이전 수준에 대한 종속 변수의 평균과 비교됩니다. 이러한 유형의 코딩은 명목 또는 순서 변수에 유용할 수 있습니다.'\n",
    "#In backward difference coding,\n",
    "# the mean of the dependent variable for a level is\n",
    "# compared with the mean of the dependent variable for the prior level.\n",
    "# This type of coding may be useful for a nominal or an ordinal variable.'\n",
    "\n",
    "#::::::::::::: SEABORN DATA PLOT START::::::::::::::::#\n",
    "fig, ax = plt.subplots(5,1,figsize=(10,20))\n",
    "for i in range(len(ordinal_cols)):\n",
    "    print(train_data[ordinal_cols[i]].value_counts())\n",
    "    sns.histplot(train_data[ordinal_cols[i]].values ,ax=ax[i])\n",
    "#::::::::::::: SEABORN DATA PLOT END::::::::::::::::#\n",
    "\n",
    "# 하나는 있는 거니까 , left merge는 있는 컬럼키면 그대로두고\n",
    "# 없는 컬럼 키면 그대로 붙이니까 merge가 가능하다"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#:::::::::::::\n",
    "\n",
    "List[Tuple[str,pd.DataFrame]]\n",
    "# type hinting , type annotation 타입에 대한 메타 정보를 제공한다 중대규모 이상프로젝트에서는 컴파일 위험 에러\n",
    " \n",
    "\n",
    "# 학습에 필요없는 컬럼 리스트\n",
    "cols_drop = [\"id\",\"person_prefer_f\",\"person_prefer_g\" ,\"contents_open_dt\", \"contents_rn\", ]\n",
    "x_train, y_train = preprocess_data(train_data, cols_merge = cols_merge , cols_equi= cols_equi , cols_drop = cols_drop)\n",
    "x_test, _ = preprocess_data(test_data,is_train = False, cols_merge = cols_merge , cols_equi= cols_equi  , cols_drop = cols_drop)\n",
    "x_train.shape , y_train.shape , x_test.shape\n",
    "x_train  = x_train.iloc[:351365,:]\n",
    "y_train = y_train[:351365]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mean_encode = x_train[[col for col in x_train.columns if 'mean_encode' in col]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#left는 그래도 on key 가 하나는 있어야 없어도 붙인다\n",
    "x_train\n",
    "mean_encode =  mean_encode.reset_index(drop=True)\n",
    "x_train = pd.concat([x_train, mean_encode],axis=1 )\n",
    "x_train.columns[x_train.nunique() > 2 ].tolist()\n",
    "# 범주형 컬럼 리스트"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train\n",
    "is_holdout = False\n",
    "n_splits = 5\n",
    "iterations = 3000\n",
    "patience= 50\n",
    "# 전역 변수\n",
    "scores = []\n",
    "models = []\n",
    "# category feature 가 아니지만 쓰였으므로\n",
    "cat_features = x_train.columns[x_train.nunique() > 2].tolist()\n",
    "SEED = 42 \n",
    "cv = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "models = []\n",
    "for tri, vai in cv.split(x_train):\n",
    "    print(\"=\"*50)\n",
    "    preds = []\n",
    "    model = CatBoostClassifier(iterations=iterations,random_state=SEED,eval_metric=\"F1\",cat_features=cat_features,one_hot_max_size=4)\n",
    "    model.fit(x_train.iloc[tri], y_train[tri],\n",
    "            eval_set=[(x_train.iloc[vai], y_train[vai])],\n",
    "            early_stopping_rounds=patience ,\n",
    "            verbose = 100\n",
    "        )\n",
    "    models.append(model)\n",
    "    scores.append(model.get_best_score()[\"validation\"][\"F1\"])\n",
    "    if is_holdout:\n",
    "        break\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(units=48, activation = 'relu' , input_shape=(68,)),\n",
    "                tf.keras.layers.Dense(units=24, activation = 'relu'),\n",
    "                tf.keras.layers.Dense(units=12, activation = 'relu'),\n",
    "                #마지막 레이어 유닛은 1로 해야 계산을 할 수 가 있다\n",
    "                tf.keras.layers.Dense(units=1 , activation = 'sigmoid')\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.07),\n",
    "              loss='binary_crossentropy' ,metrics=['accuracy'])\n",
    " \n",
    "model.summary()    \n",
    "history = model.fit(x_train , y_train , epochs=25, batch_size = 32, validation_split=0.25, callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss')])\n",
    "history"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% :::::::::::: KERAS DEEP LEARNING ::::::::::::::: #%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " \n",
    "# train start\n",
    "  \n",
    "#cat_features 를 지정하지 않아도"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% :::::::::::::::::: KERAS DEEP LEARNING END :::::::::::::::::::\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 모델에 맞지 않는 피쳐값을 넣으니 성능이 하락하네 이런 이유가 있을 수 있다\n",
    " \n",
    "# 피쳐별 가중치를 달리하는 딥러닝 모델 훈련 ? 이 의미가 있을까싶은데 내가 원하는 피쳐별 가중치를 달리하게 해주는 것은 딥러닝,용도와 의미를 이제 깨달았다\n",
    " \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}