{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "plt.style.use(['seaborn-darkgrid'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df,verbose=True):\n",
    "    numerics = ['int16','int32','int64' ,'float16','float32','float64']\n",
    "\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                if c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                if c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                if c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        end_mem = df.memory_usage().sum() / 1024 **2\n",
    "        if verbose: print(\"Mem , usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(end_mem, 100 * (start_mem - end_mem) /start_mem))\n",
    "        return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('./JobCare_data/train.csv')\n",
    "test = pd.read_csv('./JobCare_data/test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train = train.drop(['id', 'contents_open_dt'], axis=1)\n",
    "test = test.drop(['id', 'contents_open_dt'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 무지성randomforest 하지말고 시각화하고 데이터 살펴보고 뭘 적용할지결정\n",
    "\n",
    "train = train.astype(np.int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = train.iloc[:,:-1]\n",
    "y = train['target']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = x.astype(np.int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x,y , test_size=0.3 , shuffle=True, stratify=y, random_state=34)\n",
    "\n",
    "x_train = x_train.astype(np.int)\n",
    "\n",
    "# randomforest\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(x_train.values,y_train.values)\n",
    "y_pred = rf.predict(x_valid)\n",
    "rf.score(x_train,y_train)\n",
    "round(rf.score(x_train.values,y_train.values)*100,2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(y_valid.values[:100], label=\"answer\")\n",
    "plt.plot(y_pred[:100], label=\"predict\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#GridSearch\n",
    "\n",
    "rf_param_grid = {\n",
    "    'n_estimators' :  [100,200,300],\n",
    "    'max_depth' : [6,8,10,12,20,30,50,70,],\n",
    "    'min_samples_leaf' : [3,5,7,10],\n",
    "    'min_samples_split' : [2,3,5,10],\n",
    "    'n_jobs': [-1]\n",
    "}\n",
    "\n",
    "x_train_encoding = x_train.iloc[:,:-2].astype(np.int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf_grid = GridSearchCV(rf, param_grid = rf_param_grid, scoring='accuracy', n_jobs=-1 , verbose =1)\n",
    "rf_grid.fit(x_train_encoding.values , y_train.values)\n",
    "\n",
    "rf_grid.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# valid set으로 예측을 하고 score 확인 ?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train.values,y_train.values)\n",
    "y_pred = logreg.predict(x_valid)\n",
    "acc_log = round(logreg.score(x_train.values,y_train.values)* 100,2)\n",
    "# SelectFromModel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Number of features before selection : {}\".format(x_train.shape[1]))\n",
    "sfm = SelectFromModel(rf , threshold='median', prefit=True)\n",
    "n_features = sfm.transform(x_train).shape[1]\n",
    "print('Number of features after seletion : {}'.format(n_features))\n",
    "selected_vars  = list(x_train.columns[sfm.get_support()])\n",
    "preds = rf.predict(test.values)\n",
    "test = test[selected_vars]\n",
    "preds = rf.predict(test[selected_vars].values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train = train[selected_vars + ['target']]\n",
    "\n",
    "# 버릴 feature 는 버린다 많을 수록 좋은 피쳐가 아니기 때문에\n",
    "# 제출 submission"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./job_care/sample_submission.csv')\n",
    "\n",
    "submission['target'] = preds\n",
    "test.columns\n",
    "submission.to_csv('./baseline.csv', index=False)\n",
    "baseline = pd.read_csv('./baseline.csv')\n",
    "baseline['target'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# f1 score로 train에서 feature 를 가지고 학습한 모델을 test 모델에 적용해서\n",
    "# 유의미하면 ?\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#PermutationImportance\n",
    "perm = PermutationImportance(rf, scoring='accuracy', random_state=22).fit(x_valid,y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%import time\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "importances = rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time to compute the importances : {elapsed_time:.3f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#feature importance\n",
    "eli5.show_weights(perm , top =20  ,feature_names = x_valid.columns.tolist())\n",
    "# tree 기반 이므로 얼마나 트리 분할과 밀접한 관련이 있는 지를 본다\n",
    "\n",
    "# feature 하나하나 마다 shuffle하여 성능 변화 지켜보기 중요한 역할의 feature 라면 모델 서능 떨어질\n",
    "\n",
    "# weight가 양수인갑들은 중요한 값 모델에 큰 영향을 끼친다\n",
    "# contents attribute d가 중요한 featrue\n",
    "n_features = len(x_train.columns)\n",
    "\n",
    "x_train.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#LGBMClassifier\n",
    "model = lgb.LGBMClassifier(n_estimators=100, objective='binary', class_weight='balanced'\n",
    "                           ,learning_rate= 0.05, reg_alpha=0.1, reg_lambda=0.1,\n",
    "                           subsample=0.8 , n_jobs=-1 , random_state=50)\n",
    "\n",
    "model.feature_importances_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.barh(np.arange(n_features), sorted(model.feature_importances_), align='center')\n",
    "plt.yticks(np.arange(n_features) , x_train.columns)\n",
    "plt.xlabel('random forest feature importance')\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# 사용자 번호와 컨텐츠 번호는 관련이 없을 듯 한데 제거\n",
    "\n",
    "# 신경망에 리스트를 주입할 수 없으니 텐서로 변환\n",
    "\n",
    "# feature importance\n",
    "feature_names= test.columns\n",
    "forest_importances = pd.Series(importances, index=feature_names )\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title('Feature importances using MDI')\n",
    "ax.set_ylabel(\"Mena decrease in impurity\")\n",
    "fig.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# feature importance 가 높은 값에 가중치를 줘서 더 높은 정확도 필요 어떤 콘텐츠 열람하고 시청을 했느냐가 타겟\n",
    "train\n",
    "#회원 선호속성과 컨텐츠 속성과의 연관관계\n",
    "# 같은 사용자\n",
    "# 데이터 시각화 부터 하자\n",
    "train = train.astype(np.int)\n",
    "test = test.astype(np.int)\n",
    "\n",
    "# 각각 feature 간의 상관관계\n",
    "# 대분류중분류소분류가 그래도 타겟과의 상관관계가 그나마 높은 것이 보인다\n",
    "import seaborn as sns\n",
    "train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.heatmap(train.corr())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 변수 나누기\n",
    "train['d_l_match_yn']\n",
    "train = reduce_mem_usage(train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "meta_data = []\n",
    "ordinal_col = ['person_attribute_a_1','person_attribute_b','person_prefer_e','contents_attribute_e']\n",
    "for col_name in train.columns:\n",
    "    if 'yn' in col_name:\n",
    "        level = 'binary'\n",
    "    elif col_name in ordinal_col:\n",
    "        level = 'ordinal'\n",
    "    elif 'attribute' in col_name:\n",
    "        level = 'nominal'\n",
    "    elif 'prefer' in col_name:\n",
    "        level = 'nominal'\n",
    "\n",
    "    f_dict = {\n",
    "        'feature_name' : col_name,\n",
    "        'level' : level\n",
    "    }\n",
    "\n",
    "    meta_data.append(f_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "                         feature_name    level\n0                        d_l_match_yn   binary\n1                        d_m_match_yn   binary\n2                        d_s_match_yn   binary\n3                        h_l_match_yn   binary\n4                        h_m_match_yn   binary\n5                        h_s_match_yn   binary\n6                  person_attribute_a  nominal\n7                person_attribute_a_1  ordinal\n8                  person_attribute_b  ordinal\n9                     person_prefer_c  nominal\n10                  person_prefer_d_1  nominal\n11                  person_prefer_d_2  nominal\n12                  person_prefer_d_3  nominal\n13                    person_prefer_e  ordinal\n14                  person_prefer_h_1  nominal\n15                  person_prefer_h_2  nominal\n16                  person_prefer_h_3  nominal\n17               contents_attribute_i  nominal\n18               contents_attribute_a  nominal\n19             contents_attribute_j_1  nominal\n20               contents_attribute_j  nominal\n21               contents_attribute_c  nominal\n22               contents_attribute_k  nominal\n23               contents_attribute_l  nominal\n24               contents_attribute_d  nominal\n25               contents_attribute_m  nominal\n26               contents_attribute_e  ordinal\n27               contents_attribute_h  nominal\n28                             target  nominal\n29            d_l_match_yn_woe_encode   binary\n30            d_m_match_yn_woe_encode   binary\n31            d_s_match_yn_woe_encode   binary\n32            h_l_match_yn_woe_encode   binary\n33            h_m_match_yn_woe_encode   binary\n34            h_s_match_yn_woe_encode   binary\n35      person_attribute_a_woe_encode  nominal\n36    person_attribute_a_1_woe_encode  nominal\n37      person_attribute_b_woe_encode  nominal\n38         person_prefer_c_woe_encode  nominal\n39       person_prefer_d_1_woe_encode  nominal\n40       person_prefer_d_2_woe_encode  nominal\n41       person_prefer_d_3_woe_encode  nominal\n42         person_prefer_e_woe_encode  nominal\n43       person_prefer_h_1_woe_encode  nominal\n44       person_prefer_h_2_woe_encode  nominal\n45       person_prefer_h_3_woe_encode  nominal\n46    contents_attribute_i_woe_encode  nominal\n47    contents_attribute_a_woe_encode  nominal\n48  contents_attribute_j_1_woe_encode  nominal\n49    contents_attribute_j_woe_encode  nominal\n50    contents_attribute_c_woe_encode  nominal\n51    contents_attribute_k_woe_encode  nominal\n52    contents_attribute_l_woe_encode  nominal\n53    contents_attribute_d_woe_encode  nominal\n54    contents_attribute_m_woe_encode  nominal\n55    contents_attribute_e_woe_encode  nominal\n56    contents_attribute_h_woe_encode  nominal\n57                  target_woe_encode  nominal",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_name</th>\n      <th>level</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>d_l_match_yn</td>\n      <td>binary</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>d_m_match_yn</td>\n      <td>binary</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>d_s_match_yn</td>\n      <td>binary</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>h_l_match_yn</td>\n      <td>binary</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>h_m_match_yn</td>\n      <td>binary</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>h_s_match_yn</td>\n      <td>binary</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>person_attribute_a</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>person_attribute_a_1</td>\n      <td>ordinal</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>person_attribute_b</td>\n      <td>ordinal</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>person_prefer_c</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>person_prefer_d_1</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>person_prefer_d_2</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>person_prefer_d_3</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>person_prefer_e</td>\n      <td>ordinal</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>person_prefer_h_1</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>person_prefer_h_2</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>person_prefer_h_3</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>contents_attribute_i</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>contents_attribute_a</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>contents_attribute_j_1</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>contents_attribute_j</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>contents_attribute_c</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>contents_attribute_k</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>contents_attribute_l</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>contents_attribute_d</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>contents_attribute_m</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>contents_attribute_e</td>\n      <td>ordinal</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>contents_attribute_h</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>target</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>d_l_match_yn_woe_encode</td>\n      <td>binary</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>d_m_match_yn_woe_encode</td>\n      <td>binary</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>d_s_match_yn_woe_encode</td>\n      <td>binary</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>h_l_match_yn_woe_encode</td>\n      <td>binary</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>h_m_match_yn_woe_encode</td>\n      <td>binary</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>h_s_match_yn_woe_encode</td>\n      <td>binary</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>person_attribute_a_woe_encode</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>person_attribute_a_1_woe_encode</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>person_attribute_b_woe_encode</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>person_prefer_c_woe_encode</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>person_prefer_d_1_woe_encode</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>person_prefer_d_2_woe_encode</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>person_prefer_d_3_woe_encode</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>person_prefer_e_woe_encode</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>person_prefer_h_1_woe_encode</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>person_prefer_h_2_woe_encode</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>person_prefer_h_3_woe_encode</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>contents_attribute_i_woe_encode</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>contents_attribute_a_woe_encode</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>contents_attribute_j_1_woe_encode</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>contents_attribute_j_woe_encode</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>contents_attribute_c_woe_encode</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>contents_attribute_k_woe_encode</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>contents_attribute_l_woe_encode</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>contents_attribute_d_woe_encode</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>contents_attribute_m_woe_encode</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>contents_attribute_e_woe_encode</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>contents_attribute_h_woe_encode</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>target_woe_encode</td>\n      <td>nominal</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df = pd.DataFrame(meta_data, columns=['feature_name', 'level'])\n",
    "meta_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 둘다 오버샘플링 할 필요는 없다 적절한 분포\n",
    "train['target'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#####missing_data########\n",
    "vars_with_missing =  []\n",
    "\n",
    "for f in train.columns:\n",
    "    missings = train[train[f] ==-1][f].count()\n",
    "    if missings > 0:\n",
    "        vars_with_missing.append(f)\n",
    "        missings_perc = missings/train.shape[0]\n",
    "\n",
    "        print('Variable {} has {} records : ({:.2f}) with missing values'.format(f , missings , missings_perc))\n",
    "print('In total , there are {} variables with missing values'.format(len(vars_with_missing)))\n",
    "# check cardinality"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "v = meta_df[(meta_df.level == 'nominal')].index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    dist_values = train[col].value_counts().shape[0]\n",
    "    print('Variable {} has {} distinct values'.format(col, dist_values))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 변수 시각화\n",
    "train.dtypes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# target 이 1인 categorical value 에 대한 percentage\n",
    "\n",
    "for col in train.columns:\n",
    "    plt.figure()\n",
    "    fig , ax = plt.subplots(figsize=(20,10))\n",
    "    # Calculate the percnetage of target=1 per category value\n",
    "    cat_perc = train[[col,'target']].groupby([col], as_index=False).mean()\n",
    "    cat_perc.sort_values(by='target', ascending=False, inplace=True)\n",
    "    # Bar plot\n",
    "    # Order the bars decending on target mean\n",
    "    sns.barplot(ax=ax, x= col, y='target', data=cat_perc, order=cat_perc[col])\n",
    "    plt.ylabel(\"target percentage\", fontsize=18)\n",
    "    plt.xlabel(col,fontsize=18)\n",
    "    plt.tick_params(axis='both', which='major' , labelsize=10)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "col = meta_df[meta_df.level == 'nominal']['feature_name'].values.tolist()\n",
    "train = train.astype(np.int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dummification 하면 값의 수 만큼 컬럼이 늘어난다\n",
    "#pd.get_dummies(train , columns = ['person_attribute_a_1'], drop_first=True)\n",
    "# PolynomialFeature\n",
    "# 각 특성의 제곱 혹은 그 이상을 추가"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Before dummification we have {} variables in train'.format(train.shape[1]))\n",
    "train = pd.get_dummies(train, columns=col, drop_first=True)\n",
    "print('After dummification we have {} variables in train'.format(train.shape[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 분산이 너무 낮으면 제거한다\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "selector = VarianceThreshold(threshold=.01)\n",
    "\n",
    "\n",
    "train\n",
    "\n",
    "# 이진 변수 이고 nomial변수인데 분산이 필요한가?\n",
    "selector.fit(train.drop(['target','person_rn','contents_rn']))\n",
    "\n",
    "train_select = train.select_dtypes(include=['int'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mutual_info_classif(train_select.values , train.target.values, n_neighbors=3, random_state=17)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_select.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "####randomforest 2\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf= RandomForestClassifier(n_estimators=150 , max_depth=8 , min_samples_split=4, max_features=0.2, n_jobs=-1 ,random_state=0)\n",
    "rf.fit(train,train.target)\n",
    "\n",
    "features = train.columns[:-1].values\n",
    "\n",
    "trace = go.Scatter(\n",
    "    y=rf.feature_importances_,\n",
    "    x= features,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        sizemode='diameter',\n",
    "        sizeref =1 ,\n",
    "        size= 13,\n",
    "        color= rf.feature_importances_,\n",
    "        colorscale='Portland',\n",
    "        showscale=True\n",
    "    ),\n",
    "    text= features\n",
    ")\n",
    "data = [trace]\n",
    "\n",
    "layout = go.Layout(\n",
    "    autosize=True,\n",
    "    title='Random Forest Feature Importance',\n",
    "    hovermode='closest',\n",
    "    xaxis = dict(\n",
    "        ticklen=5,\n",
    "        showgrid=False ,\n",
    "        zeroline=False,\n",
    "        showline=False\n",
    "    ),\n",
    "    yaxis = dict(\n",
    "        title = 'Feature Importance',\n",
    "        showgrid =False,\n",
    "        zeroline=False,\n",
    "        ticklen=5,\n",
    "        gridwidth=2\n",
    "    ),\n",
    "    showlegend=False\n",
    ")\n",
    "fig = go.Figure(data=data,layout=layout)\n",
    "py.iplot(fig,filename='scatter2010')\n",
    "\n",
    "\n",
    "x, y =list((x) for x in zip(*sorted(zip(rf.feature_importances_,features), reverse=False)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trace2 = go.Bar(\n",
    "    x=x ,\n",
    "    y=y,\n",
    "    marker=dict(\n",
    "        color=x ,\n",
    "        colorscale = 'Viridis',\n",
    "        reversescale=True\n",
    "    ),\n",
    "    name='Random Forest Feature Importance',\n",
    "    orientation='h',\n",
    ")\n",
    "layout= dict(\n",
    "    title='Barplot of Feature importances',\n",
    "    width= 900, height = 2000,\n",
    "    yaxis =dict(\n",
    "        showgrid=False ,\n",
    "        showline=False,\n",
    "        showticklabels=True,\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# barplot 이 훨씬 시각적으로 와닿는다\n",
    "\n",
    "fig1 = go.Figure(data=[trace2])\n",
    "fig1['layout'].update(layout)\n",
    "py.iplot(fig1, filename='plots')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from IPython.display import Image as PImage\n",
    "from subprocess import check_call\n",
    "from PIL import Image , ImageDraw , ImageFont\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train.iloc[:,:-1].astype(np.int).drop(['person_rn','contents_rn'],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target = train['target']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = train.iloc[:,:-1].astype(np.int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x,y , test_size=0.3 , shuffle=True, stratify=y, random_state=34)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#desiciontreeregressor  를 하면 안되지 .. logisticregresoor도 아니고"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### DecisionTreeClassifier #####\n",
    "decision_tree = tree.DecisionTreeClassifier(max_depth=3)\n",
    "decision_tree.fit(x_train, y_train.values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = decision_tree.predict(x_valid)\n",
    "\n",
    "import sklearn.metrics as mt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# accuracy = TP + TN / TP + TN + FP + FN (전체)\n",
    "accuracy = mt.accuracy_score(y_valid,y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 어느 피쳐가 들어갈 때 성능이 많이 떨어질까?\n",
    "# 모든 피쳐 조합 ?\n",
    "\n",
    "# 여부 feature 로만 모델 테스트 해보기"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "yn_col = []\n",
    "for col in cols:\n",
    "    if 'match_yn' in col:\n",
    "        yn_col.append(col)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "### GradientBoostingClassifier\n",
    "#split마다 고려되는 features의 수 float이면 int(max_features * n_features)값이다\n",
    "yn_train = train.loc[:,yn_col].astype(np.int)\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(yn_train,target,stratify=target)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=100, max_depth=3, min_samples_leaf=4, max_features=0.2, random_state=0)\n",
    "gb.fit(x_train,y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "(y_valid == gb.predict(x_valid)).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gb_param_grid = {\n",
    "    'n_estimators' :  [100,200],\n",
    "    'max_depth' : [6,8],\n",
    "    'min_samples_leaf' : [3,5],\n",
    "    'min_samples_split' : [2,3]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf_grid = GridSearchCV(gb, param_grid = gb_param_grid, scoring='accuracy', n_jobs=-1 , verbose =1)\n",
    "rf_grid.fit(x_train,y_train)\n",
    "\n",
    "rf_grid.best_params_\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "attribute_col = []\n",
    "for col in cols:\n",
    "    if 'attribute' in col:\n",
    "        attribute_col.append(col)\n",
    "attribute_train = train.loc[:,attribute_col].astype(np.int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(attribute_train,target,stratify=target)\n",
    "\n",
    "\n",
    "\n",
    "prefer_col = []\n",
    "for col in cols:\n",
    "    if 'prefer' in col:\n",
    "        prefer_col.append(col)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prefer_train = train.loc[:, prefer_col].astype(np.int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 여부, 선호 , 속성 각각을 트레이닝 해보고 조합도 트레이닝 해본다 ? 별로 좋은 것 같진 않지만"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 선호는 확실히 모델 성능이 떨어지기는 한다"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(prefer_train,target,stratify=target)\n",
    "\n",
    "\n",
    "target = train['target']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(yn_train,target,stratify=target)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train = x_train.astype(np.int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gb.fit(x_train,y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 0.569면 비슷하다 그냥 이것도\n",
    "(y_valid == gb.predict(x_valid)).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train , eval_metric='auc' , eval_set=[(x_valid,y_valid) , (x_train,y_train)],\n",
    "         eval_names=['valid','train'],early_stopping_rounds=100, verbose=200\n",
    "          )\n",
    "\n",
    "\n",
    "\n",
    "x_test = test.loc[: ,yn_col].astype(np.int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(gb.predict(x_test) == model.predict(x_test)).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### decision tree 시각화 #####\n",
    "# 덮어쓰기 안되므로 트레이닝 후 수동으로 삭제 후 다시 저자\n",
    "# Export our trained model as a .dot file\n",
    "with open(\"tree1.dot\", 'w') as f:\n",
    "    f = tree.export_graphviz(decision_tree,\n",
    "                            out_file=f,\n",
    "                            max_depth=4,\n",
    "                            impurity= False,\n",
    "                            feature_names = train.iloc[:,:-1].columns.values.tolist(),\n",
    "                            class_names =['No' , 'Yes'],\n",
    "                            rounded =True,\n",
    "                            filled=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from subprocess import check_call\n",
    "\n",
    "# 정확도가 많이 낮네\n",
    "(y_valid == decision_tree.predict(x_valid)).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert .dot to .png to allow display in web notebook\n",
    "check_call(['dot', '-Tpng', 'tree1.dot' , '-o','tree1.png'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Annotating chart with PIL\n",
    "img = Image.open(\"tree1.png\")\n",
    "draw = ImageDraw.Draw(img)\n",
    "img.save(\"sample-out.png\")\n",
    "PImage(\"sample-out.png\")\n",
    "\n",
    "\n",
    "import os\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(mf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "v = train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "print('Before dummification we have {} variables in train')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# category 2 is not twice the value of category 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train['person_prefer_d_1']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 같은 컨텐츠\n",
    "train['contents_rn'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "########## TabNet #############\n",
    "from torch import nn\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./JobCare_data/train.csv')\n",
    "test_df = pd.read_csv('./JobCare_data/test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "train = train_df[train_df['contents_open_dt'].apply(lambda x: pd.Timestamp(x).month)<11].copy()\n",
    "val = train_df[train_df['contents_open_dt'].apply(lambda x: pd.Timestamp(x).month)==11].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "        d_l_match_yn  d_m_match_yn  d_s_match_yn  h_l_match_yn  h_m_match_yn  \\\n0               True          True          True         False         False   \n1              False         False         False          True          True   \n2              False         False         False          True         False   \n3              False         False         False          True         False   \n4               True          True          True         False         False   \n...              ...           ...           ...           ...           ...   \n501946         False         False         False          True         False   \n501947          True          True         False          True         False   \n501948          True          True          True          True         False   \n501949          True         False         False          True         False   \n501950          True          True          True          True         False   \n\n        h_s_match_yn  person_attribute_a  person_attribute_a_1  \\\n0              False                   1                     4   \n1              False                   1                     3   \n2              False                   2                     0   \n3              False                   2                     0   \n4              False                   1                     3   \n...              ...                 ...                   ...   \n501946         False                   1                     1   \n501947         False                   1                     6   \n501948         False                   1                     7   \n501949         False                   1                     1   \n501950         False                   1                     6   \n\n        person_attribute_b  person_prefer_c  ...  \\\n0                        3                5  ...   \n1                        4                1  ...   \n2                        3                5  ...   \n3                        2                5  ...   \n4                        4                5  ...   \n...                    ...              ...  ...   \n501946                   2                2  ...   \n501947                   2                1  ...   \n501948                   4                1  ...   \n501949                   2                1  ...   \n501950                   3                5  ...   \n\n        contents_attribute_j_1_woe_encode  contents_attribute_j_woe_encode  \\\n0                               -0.110406                        -0.131229   \n1                                0.110187                         0.039526   \n2                               -0.110406                        -0.131229   \n3                                0.110187                         0.039526   \n4                               -0.110406                        -0.131229   \n...                                   ...                              ...   \n501946                           0.110187                         0.039526   \n501947                          -0.110406                        -0.131229   \n501948                           0.110187                         0.039526   \n501949                           0.110187                         0.039526   \n501950                           0.110187                         0.039526   \n\n        contents_attribute_c_woe_encode  contents_attribute_k_woe_encode  \\\n0                              0.050647                         0.008244   \n1                              0.050647                         0.008244   \n2                              0.050647                        -0.245994   \n3                              0.050647                         0.008244   \n4                              0.050647                         0.008244   \n...                                 ...                              ...   \n501946                         0.050647                         0.008244   \n501947                         0.050647                         0.008244   \n501948                         0.050647                         0.008244   \n501949                         0.050647                         0.008244   \n501950                         0.050647                         0.008244   \n\n        contents_attribute_l_woe_encode  contents_attribute_d_woe_encode  \\\n0                              0.004922                        -0.331832   \n1                              0.004922                        -0.331832   \n2                              0.304402                        -0.061951   \n3                              0.004922                        -0.331832   \n4                              0.004922                        -0.331832   \n...                                 ...                              ...   \n501946                         0.137717                         0.218158   \n501947                        -0.037998                         0.377821   \n501948                        -0.043894                         0.218158   \n501949                         0.448950                         0.218158   \n501950                         0.140357                         0.218158   \n\n        contents_attribute_m_woe_encode  contents_attribute_e_woe_encode  \\\n0                              0.032566                         0.016421   \n1                              0.032566                         0.016421   \n2                              0.032566                         0.016421   \n3                              0.048236                        -0.005555   \n4                              0.032566                         0.016421   \n...                                 ...                              ...   \n501946                         0.032566                         0.024290   \n501947                         0.032566                         0.016421   \n501948                         0.000910                         0.113906   \n501949                        -0.028532                         0.016421   \n501950                         0.000910                         0.024290   \n\n        contents_attribute_h_woe_encode  target_woe_encode  \n0                             -0.147974          13.815511  \n1                              0.174218               -inf  \n2                             -0.159230               -inf  \n3                             -0.001587               -inf  \n4                             -0.001587               -inf  \n...                                 ...                ...  \n501946                         0.142156          13.815511  \n501947                        -0.248896          13.815511  \n501948                         0.142156          13.815511  \n501949                        -0.081789          13.815511  \n501950                        -0.028092          13.815511  \n\n[456972 rows x 63 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>d_l_match_yn</th>\n      <th>d_m_match_yn</th>\n      <th>d_s_match_yn</th>\n      <th>h_l_match_yn</th>\n      <th>h_m_match_yn</th>\n      <th>h_s_match_yn</th>\n      <th>person_attribute_a</th>\n      <th>person_attribute_a_1</th>\n      <th>person_attribute_b</th>\n      <th>person_prefer_c</th>\n      <th>...</th>\n      <th>contents_attribute_j_1_woe_encode</th>\n      <th>contents_attribute_j_woe_encode</th>\n      <th>contents_attribute_c_woe_encode</th>\n      <th>contents_attribute_k_woe_encode</th>\n      <th>contents_attribute_l_woe_encode</th>\n      <th>contents_attribute_d_woe_encode</th>\n      <th>contents_attribute_m_woe_encode</th>\n      <th>contents_attribute_e_woe_encode</th>\n      <th>contents_attribute_h_woe_encode</th>\n      <th>target_woe_encode</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>1</td>\n      <td>4</td>\n      <td>3</td>\n      <td>5</td>\n      <td>...</td>\n      <td>-0.110406</td>\n      <td>-0.131229</td>\n      <td>0.050647</td>\n      <td>0.008244</td>\n      <td>0.004922</td>\n      <td>-0.331832</td>\n      <td>0.032566</td>\n      <td>0.016421</td>\n      <td>-0.147974</td>\n      <td>13.815511</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.110187</td>\n      <td>0.039526</td>\n      <td>0.050647</td>\n      <td>0.008244</td>\n      <td>0.004922</td>\n      <td>-0.331832</td>\n      <td>0.032566</td>\n      <td>0.016421</td>\n      <td>0.174218</td>\n      <td>-inf</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>5</td>\n      <td>...</td>\n      <td>-0.110406</td>\n      <td>-0.131229</td>\n      <td>0.050647</td>\n      <td>-0.245994</td>\n      <td>0.304402</td>\n      <td>-0.061951</td>\n      <td>0.032566</td>\n      <td>0.016421</td>\n      <td>-0.159230</td>\n      <td>-inf</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>5</td>\n      <td>...</td>\n      <td>0.110187</td>\n      <td>0.039526</td>\n      <td>0.050647</td>\n      <td>0.008244</td>\n      <td>0.004922</td>\n      <td>-0.331832</td>\n      <td>0.048236</td>\n      <td>-0.005555</td>\n      <td>-0.001587</td>\n      <td>-inf</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n      <td>5</td>\n      <td>...</td>\n      <td>-0.110406</td>\n      <td>-0.131229</td>\n      <td>0.050647</td>\n      <td>0.008244</td>\n      <td>0.004922</td>\n      <td>-0.331832</td>\n      <td>0.032566</td>\n      <td>0.016421</td>\n      <td>-0.001587</td>\n      <td>-inf</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>501946</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.110187</td>\n      <td>0.039526</td>\n      <td>0.050647</td>\n      <td>0.008244</td>\n      <td>0.137717</td>\n      <td>0.218158</td>\n      <td>0.032566</td>\n      <td>0.024290</td>\n      <td>0.142156</td>\n      <td>13.815511</td>\n    </tr>\n    <tr>\n      <th>501947</th>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>1</td>\n      <td>6</td>\n      <td>2</td>\n      <td>1</td>\n      <td>...</td>\n      <td>-0.110406</td>\n      <td>-0.131229</td>\n      <td>0.050647</td>\n      <td>0.008244</td>\n      <td>-0.037998</td>\n      <td>0.377821</td>\n      <td>0.032566</td>\n      <td>0.016421</td>\n      <td>-0.248896</td>\n      <td>13.815511</td>\n    </tr>\n    <tr>\n      <th>501948</th>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>1</td>\n      <td>7</td>\n      <td>4</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.110187</td>\n      <td>0.039526</td>\n      <td>0.050647</td>\n      <td>0.008244</td>\n      <td>-0.043894</td>\n      <td>0.218158</td>\n      <td>0.000910</td>\n      <td>0.113906</td>\n      <td>0.142156</td>\n      <td>13.815511</td>\n    </tr>\n    <tr>\n      <th>501949</th>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.110187</td>\n      <td>0.039526</td>\n      <td>0.050647</td>\n      <td>0.008244</td>\n      <td>0.448950</td>\n      <td>0.218158</td>\n      <td>-0.028532</td>\n      <td>0.016421</td>\n      <td>-0.081789</td>\n      <td>13.815511</td>\n    </tr>\n    <tr>\n      <th>501950</th>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>1</td>\n      <td>6</td>\n      <td>3</td>\n      <td>5</td>\n      <td>...</td>\n      <td>0.110187</td>\n      <td>0.039526</td>\n      <td>0.050647</td>\n      <td>0.008244</td>\n      <td>0.140357</td>\n      <td>0.218158</td>\n      <td>0.000910</td>\n      <td>0.024290</td>\n      <td>-0.028092</td>\n      <td>13.815511</td>\n    </tr>\n  </tbody>\n</table>\n<p>456972 rows × 63 columns</p>\n</div>"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "test= test_df.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "for df in [train,val,test]:\n",
    "    if 'contents_open_dt' in df.columns:\n",
    "        df.drop(['contents_open_dt'],axis=1,inplace=True)\n",
    "    if 'contents_rn' in df.columns:\n",
    "        df.drop(['contents_rn'],axis=1 , inplace=True)\n",
    "    if 'id' in df.columns:\n",
    "        df.drop(['id'],axis=1 , inplace=True)\n",
    "    if 'person_rn' in df.columns:\n",
    "        df.drop(['person_rn'],axis=1 , inplace=True)\n",
    "    if 'person_prefer_f' in df.columns:\n",
    "        df.drop(['person_prefer_f'],axis=1 , inplace=True)\n",
    "    if 'person_prefer_g' in df.columns:\n",
    "        df.drop(['person_prefer_g'],axis=1 , inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "columns = sorted(test.columns)\n",
    "train = train[columns+ ['target']] *1\n",
    "val = val[columns+['target']]*1\n",
    "test = test[columns] * 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "cat_idxs = []\n",
    "cat_dims = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df[col].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "data": {
      "text/plain": "0    251106\n1    250845\nName: target, dtype: int64"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[col].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df['contents_attribute_h'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train[col]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# le_dict 가 2~313 을 key로 가지고 있다면 컬럼에서 2~313을 가지고 있는 컬럼에서 매핑을 한것이다\n",
    "# labelencoder 는 말 그대로 2~313"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['d_l_match_yn', 'd_m_match_yn', 'd_s_match_yn', 'h_l_match_yn',\n       'h_m_match_yn', 'h_s_match_yn', 'person_attribute_a',\n       'person_attribute_a_1', 'person_attribute_b', 'person_prefer_c',\n       'person_prefer_d_1', 'person_prefer_d_2', 'person_prefer_d_3',\n       'person_prefer_e', 'person_prefer_f', 'person_prefer_g',\n       'person_prefer_h_1', 'person_prefer_h_2', 'person_prefer_h_3',\n       'contents_attribute_i', 'contents_attribute_a',\n       'contents_attribute_j_1', 'contents_attribute_j',\n       'contents_attribute_c', 'contents_attribute_k', 'contents_attribute_l',\n       'contents_attribute_d', 'contents_attribute_m', 'contents_attribute_e',\n       'contents_attribute_h', 'contents_open_dt', 'target',\n       'd_l_match_yn_woe_encode', 'd_m_match_yn_woe_encode',\n       'd_s_match_yn_woe_encode', 'h_l_match_yn_woe_encode',\n       'h_m_match_yn_woe_encode', 'h_s_match_yn_woe_encode',\n       'person_attribute_a_woe_encode', 'person_attribute_a_1_woe_encode',\n       'person_attribute_b_woe_encode', 'person_prefer_c_woe_encode',\n       'person_prefer_d_1_woe_encode', 'person_prefer_d_2_woe_encode',\n       'person_prefer_d_3_woe_encode', 'person_prefer_e_woe_encode',\n       'person_prefer_f_woe_encode', 'person_prefer_g_woe_encode',\n       'person_prefer_h_1_woe_encode', 'person_prefer_h_2_woe_encode',\n       'person_prefer_h_3_woe_encode', 'contents_attribute_i_woe_encode',\n       'contents_attribute_a_woe_encode', 'contents_attribute_j_1_woe_encode',\n       'contents_attribute_j_woe_encode', 'contents_attribute_c_woe_encode',\n       'contents_attribute_k_woe_encode', 'contents_attribute_l_woe_encode',\n       'contents_attribute_d_woe_encode', 'contents_attribute_m_woe_encode',\n       'contents_attribute_e_woe_encode', 'contents_attribute_h_woe_encode',\n       'target_woe_encode'],\n      dtype='object')"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['d_l_match_yn', 'd_m_match_yn', 'd_s_match_yn', 'h_l_match_yn',\n       'h_m_match_yn', 'h_s_match_yn', 'person_attribute_a',\n       'person_attribute_a_1', 'person_attribute_b', 'person_prefer_c',\n       'person_prefer_d_1', 'person_prefer_d_2', 'person_prefer_d_3',\n       'person_prefer_e', 'person_prefer_f', 'person_prefer_g',\n       'person_prefer_h_1', 'person_prefer_h_2', 'person_prefer_h_3',\n       'contents_attribute_i', 'contents_attribute_a',\n       'contents_attribute_j_1', 'contents_attribute_j',\n       'contents_attribute_c', 'contents_attribute_k', 'contents_attribute_l',\n       'contents_attribute_d', 'contents_attribute_m', 'contents_attribute_e',\n       'contents_attribute_h', 'contents_open_dt', 'target',\n       'd_l_match_yn_woe_encode', 'd_m_match_yn_woe_encode',\n       'd_s_match_yn_woe_encode', 'h_l_match_yn_woe_encode',\n       'h_m_match_yn_woe_encode', 'h_s_match_yn_woe_encode',\n       'person_attribute_a_woe_encode', 'person_attribute_a_1_woe_encode',\n       'person_attribute_b_woe_encode', 'person_prefer_c_woe_encode',\n       'person_prefer_d_1_woe_encode', 'person_prefer_d_2_woe_encode',\n       'person_prefer_d_3_woe_encode', 'person_prefer_e_woe_encode',\n       'person_prefer_f_woe_encode', 'person_prefer_g_woe_encode',\n       'person_prefer_h_1_woe_encode', 'person_prefer_h_2_woe_encode',\n       'person_prefer_h_3_woe_encode', 'contents_attribute_i_woe_encode',\n       'contents_attribute_a_woe_encode', 'contents_attribute_j_1_woe_encode',\n       'contents_attribute_j_woe_encode', 'contents_attribute_c_woe_encode',\n       'contents_attribute_k_woe_encode', 'contents_attribute_l_woe_encode',\n       'contents_attribute_d_woe_encode', 'contents_attribute_m_woe_encode',\n       'contents_attribute_e_woe_encode', 'contents_attribute_h_woe_encode',\n       'target_woe_encode'],\n      dtype='object')"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for idx, col in enumerate(train.columns):\n",
    "    if 'match' not in col and col != 'target':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(train_df[col].values)\n",
    "        le_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "        train[col] = train[col].apply(lambda x: le_dict.get(x,len(le_dict)))\n",
    "        val[col] = val[col].apply(lambda x: le_dict.get(x, len(le_dict)))\n",
    "        test[col] = test[col].apply(lambda x: le_dict.get(x, len(le_dict)))\n",
    "        cat_idxs.append(idx)\n",
    "        cat_dims.append(len(le_dict)+1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = train.drop('target',axis=1).values\n",
    "y_train = train['target'].values\n",
    "X_val = val.drop('target', axis=1).values\n",
    "y_val = val['target'].values\n",
    "X_test = test.values\n",
    "eval_set = (X_val, y_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cat_idxs = []\n",
    "cat_dims = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'person_prefer_f_woe_encode'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001B[0m in \u001B[0;36mget_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3079\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3080\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3081\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'person_prefer_f_woe_encode'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/c6/2j22b1rd02778st12c0mcr3w0000gn/T/ipykernel_828/2030033836.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m         \u001B[0mtrain\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mle_dict\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mle_dict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m         \u001B[0mval\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mval\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mle_dict\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mle_dict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m         \u001B[0mtest\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtest\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mle_dict\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mle_dict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m         \u001B[0mcat_idxs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m         \u001B[0mcat_dims\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mle_dict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/pandas/core/frame.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3022\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnlevels\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3023\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_getitem_multilevel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3024\u001B[0;31m             \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3025\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mis_integer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindexer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3026\u001B[0m                 \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mindexer\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001B[0m in \u001B[0;36mget_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3080\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3081\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3082\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3083\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3084\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtolerance\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'person_prefer_f_woe_encode'"
     ]
    }
   ],
   "source": [
    "for idx, col in enumerate(train.columns):\n",
    "    if 'match' not in col and col != 'target':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(train_df[col].values)\n",
    "        le_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "        train[col] = train[col].apply(lambda x: le_dict.get(x,len(le_dict)))\n",
    "        val[col] = val[col].apply(lambda x: le_dict.get(x, len(le_dict)))\n",
    "        test[col] = test[col].apply(lambda x: le_dict.get(x, len(le_dict)))\n",
    "        cat_idxs.append(idx)\n",
    "        cat_dims.append(len(le_dict)+1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf = TabNetClassifier(cat_idxs=cat_idxs,\n",
    "                       cat_dims=cat_dims,\n",
    "                       cat_emb_dim=5,\n",
    "                       optimizer_fn=torch.optim.AdamW,# Any optimizer work here\n",
    "                       mask_type='entmax',#\"spaesemax\")\n",
    "                       )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class F1_Score(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"f1\"\n",
    "        self._maximize = True\n",
    "    def __call__(self, y_true, y_score):\n",
    "        score = f1_score(y_true, (y_score[:,1]>0.5)* 1)\n",
    "        return score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_valid.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#이건 어떻게 하길래 더 높은 정확도가 나올까\n",
    "clf.fit(\n",
    "    X_train= x_train.values, y_train=y_train.values,\n",
    "    eval_set=[(x_train.values, y_train.values), (x_valid.values, y_valid.values)],\n",
    "    eval_name=['train', 'val'],\n",
    "    eval_metric=['logloss','f1'],\n",
    "    max_epochs=100, patience=2,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=256,\n",
    "    num_workers=1,\n",
    "    drop_last=False,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preds = clf.predict_proba(x_test)\n",
    "preds = (preds[:,1]>0.5)*1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./job_care/sample_submission.csv')\n",
    "submission['target'] = preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "\n",
    " #%%\n",
    "train = pd.read_csv('./JobCare_data/train.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train[['person_attribute_a_1','person_attribute_b','person_attribute_b','contents_attribute_e']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### feature encoding #####\n",
    "attr_a_1_mean_encode = train.groupby('person_attribute_a_1')[\"target\"].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "if 'attribute' in col:\n",
    "    name = 'person_attribute_a_1'['person_attribute_a_1'.index('attribute') + len('attribute')+1:]\n",
    "elif 'prefer' in col:\n",
    "    name ='person_attribute_a_1'['person_attribute_a_1'.index('attribute') + len('attribute')+1:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nominal_cols[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [],
   "source": [
    "nominal_cols = ['person_attribute_a_1','person_attribute_b','person_prefer_e','contents_attribute_e']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 오버피팅이 자주 발생하는 mean encoding 이므로 cross validation 과 정규화 같이 사용한다=\n",
    "# 변환 하고자 하는 범주형 변수 선택\n",
    "# 범주형 변수 그룹화 -> 타깃 변수 총합 합계\n",
    "# 범주형 변수 그룹화 타깃 빈도수 합계\n",
    "# 총합을 카운트로 나누고 본래 범주 값에 업데이트\n",
    "# 여러가지 방법으로 적용 가능하다\n",
    "# 비슷한 범주 사이에 있는 관계 표현 특징, 범주와 타깃사이에만 국한된다\n",
    "# 범주가 많은 경우 이 방법은 데이터를 훨씬 더 단순화 한다"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target = train.target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [
    "for col in nominal_cols:\n",
    "    if 'attribute' in col:\n",
    "        name = col[col.index('attribute') + len('attribute')+1:]\n",
    "        var_name = 'attr_{}_mean_encode'.format(name)\n",
    "        locals()[var_name] = train.groupby(col)[\"target\"].mean()\n",
    "    elif 'prefer' in col:\n",
    "        name = col[col.index('prefer') + len('prefer')+1:]\n",
    "        var_name = 'prefer_{}_mean_encode'.format(name)\n",
    "        locals()[var_name] = train.groupby(col)[\"target\"].mean()\n",
    "\n",
    "    train.loc[:,var_name] = train[col].map(locals()[var_name])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [],
   "source": [
    "train = train.drop('target',axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(train,target , test_size=0.3 , shuffle=True, stratify=target, random_state=34)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2020-06-28 23:27:49'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/c6/2j22b1rd02778st12c0mcr3w0000gn/T/ipykernel_828/1754752561.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mrf\u001B[0m\u001B[0;34m=\u001B[0m \u001B[0mRandomForestClassifier\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn_estimators\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m150\u001B[0m \u001B[0;34m,\u001B[0m \u001B[0mmax_depth\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m8\u001B[0m \u001B[0;34m,\u001B[0m \u001B[0mmin_samples_split\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_features\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_jobs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m \u001B[0;34m,\u001B[0m\u001B[0mrandom_state\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mrf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_train\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0my_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m    302\u001B[0m                 \u001B[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    303\u001B[0m             )\n\u001B[0;32m--> 304\u001B[0;31m         X, y = self._validate_data(X, y, multi_output=True,\n\u001B[0m\u001B[1;32m    305\u001B[0m                                    accept_sparse=\"csc\", dtype=DTYPE)\n\u001B[1;32m    306\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0msample_weight\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/sklearn/base.py\u001B[0m in \u001B[0;36m_validate_data\u001B[0;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[1;32m    431\u001B[0m                 \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcheck_array\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mcheck_y_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    432\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 433\u001B[0;31m                 \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcheck_X_y\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mcheck_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    434\u001B[0m             \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    435\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36minner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     61\u001B[0m             \u001B[0mextra_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mall_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m             \u001B[0;31m# extra_args > 0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36mcheck_X_y\u001B[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[1;32m    869\u001B[0m         \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"y cannot be None\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    870\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 871\u001B[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001B[0m\u001B[1;32m    872\u001B[0m                     \u001B[0maccept_large_sparse\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maccept_large_sparse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    873\u001B[0m                     \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36minner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     61\u001B[0m             \u001B[0mextra_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mall_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m             \u001B[0;31m# extra_args > 0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36mcheck_array\u001B[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001B[0m\n\u001B[1;32m    671\u001B[0m                     \u001B[0marray\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0marray\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcasting\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"unsafe\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    672\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 673\u001B[0;31m                     \u001B[0marray\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    674\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mComplexWarning\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mcomplex_warning\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    675\u001B[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001B[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/numpy/core/_asarray.py\u001B[0m in \u001B[0;36masarray\u001B[0;34m(a, dtype, order, like)\u001B[0m\n\u001B[1;32m    100\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_asarray_with_like\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlike\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlike\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 102\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morder\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    103\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    104\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36m__array__\u001B[0;34m(self, dtype)\u001B[0m\n\u001B[1;32m   1897\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1898\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__array__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndarray\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1899\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_values\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1900\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1901\u001B[0m     def __array_wrap__(\n",
      "\u001B[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/numpy/core/_asarray.py\u001B[0m in \u001B[0;36masarray\u001B[0;34m(a, dtype, order, like)\u001B[0m\n\u001B[1;32m    100\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_asarray_with_like\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlike\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlike\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 102\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morder\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    103\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    104\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: could not convert string to float: '2020-06-28 23:27:49'"
     ]
    }
   ],
   "source": [
    "rf= RandomForestClassifier(n_estimators=150 , max_depth=8 , min_samples_split=4, max_features=0.2, n_jobs=-1 ,random_state=0)\n",
    "\n",
    "rf.fit(x_train,y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_valid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/c6/2j22b1rd02778st12c0mcr3w0000gn/T/ipykernel_828/3195843270.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# 과적합일진 모르지만 그래도많이올랐따 만족할 수 없음\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0;34m(\u001B[0m\u001B[0my_valid\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mrf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_valid\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'y_valid' is not defined"
     ]
    }
   ],
   "source": [
    "# 과적합일진 모르지만 그래도많이올랐따 만족할 수 없음\n",
    "(y_valid == rf.predict(x_valid)).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['d_l_match_yn', 'd_m_match_yn', 'd_s_match_yn', 'h_l_match_yn',\n       'h_m_match_yn', 'h_s_match_yn', 'person_attribute_a',\n       'person_attribute_a_1', 'person_attribute_b', 'person_prefer_c',\n       'person_prefer_d_1', 'person_prefer_d_2', 'person_prefer_d_3',\n       'person_prefer_e', 'person_prefer_h_1', 'person_prefer_h_2',\n       'person_prefer_h_3', 'contents_attribute_i', 'contents_attribute_a',\n       'contents_attribute_j_1', 'contents_attribute_j',\n       'contents_attribute_c', 'contents_attribute_k', 'contents_attribute_l',\n       'contents_attribute_d', 'contents_attribute_m', 'contents_attribute_e',\n       'contents_attribute_h', 'target', 'd_l_match_yn_woe_encode',\n       'd_m_match_yn_woe_encode', 'd_s_match_yn_woe_encode',\n       'h_l_match_yn_woe_encode', 'h_m_match_yn_woe_encode',\n       'h_s_match_yn_woe_encode', 'person_attribute_a_woe_encode',\n       'person_attribute_a_1_woe_encode', 'person_attribute_b_woe_encode',\n       'person_prefer_c_woe_encode', 'person_prefer_d_1_woe_encode',\n       'person_prefer_d_2_woe_encode', 'person_prefer_d_3_woe_encode',\n       'person_prefer_e_woe_encode', 'person_prefer_f_woe_encode',\n       'person_prefer_g_woe_encode', 'person_prefer_h_1_woe_encode',\n       'person_prefer_h_2_woe_encode', 'person_prefer_h_3_woe_encode',\n       'contents_attribute_i_woe_encode', 'contents_attribute_a_woe_encode',\n       'contents_attribute_j_1_woe_encode', 'contents_attribute_j_woe_encode',\n       'contents_attribute_c_woe_encode', 'contents_attribute_k_woe_encode',\n       'contents_attribute_l_woe_encode', 'contents_attribute_d_woe_encode',\n       'contents_attribute_m_woe_encode', 'contents_attribute_e_woe_encode',\n       'contents_attribute_h_woe_encode', 'target_woe_encode'],\n      dtype='object')"
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "###### Smooth Encoding ##########\n",
    "\n",
    "# 위와 다르게 smooth한 평균을 계산하고 적용하는 방법\n",
    "# 1. 평균을 계산\n",
    "Mean = train['target'].mean()\n",
    "weight = 100\n",
    "for col in nominal_cols:\n",
    "\n",
    "    if 'attribute' in col:\n",
    "        name = col[col.index('attribute') + len('attribute')+1:]\n",
    "        var_name = 'attr_{}_mean_encode'.format(name)\n",
    "\n",
    "    elif 'prefer' in col:\n",
    "        name = col[col.index('prefer') + len('prefer')+1:]\n",
    "        var_name = 'prefer_{}_mean_encode'.format(name)\n",
    "\n",
    "    # 2. 각 그룹에 대한 값들의 빈도와 평균을 계산\n",
    "    Agg = train.groupby(col)['target'].agg(['count','mean'])\n",
    "    counts = Agg['count']\n",
    "    #%%\n",
    "    means = Agg['mean']\n",
    "\n",
    "    # 3. 'smooth'한 평균을 계산\n",
    "    smooth = (counts * means + weight * means) / (counts+weight)\n",
    "    print(smooth)\n",
    "\n",
    "    # smooth한 평균에 따라 각 값을 대체하는 것\n",
    "    train.loc[:,'smooth_'+var_name] = train[col].map(smooth)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train=train.drop(['smoothattr_a_1_mean_encode',\t'smoothattr_b_mean_encode'\t,'smoothprefer_e_mean_encode'\t,'smoothattr_e_mean_encode'],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train  =train.drop(['attr_a_1_mean_encode','attr_b_mean_encode','prefer_e_mean_encode','attr_e_mean_encode','target'],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "plt.style.use(['seaborn-darkgrid'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train.info()\n",
    "train.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = train.iloc[:,:-1]\n",
    "y = train['target']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = x.astype(np.int)\n",
    "\n",
    "x_train = x_train.astype(np.int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# valid set으로 예측을 하고 score 확인 ?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train.values,y_train.values)\n",
    "y_pred = logreg.predict(x_valid)\n",
    "\n",
    "\n",
    "acc_log = round(logreg.score(x_train.values,y_train.values)* 100,2)\n",
    "\n",
    "\n",
    "# 버릴 feature 는 버린다 많을 수록 좋은 피쳐가 아니기 때문에\n",
    "# 제출 submission"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###### 제출 코드 #####\n",
    "submission = pd.read_csv('./job_care/sample_submission.csv')\n",
    "\n",
    "submission['target'] = preds\n",
    "test.columns\n",
    "submission.to_csv('./baseline.csv', index=False)\n",
    "baseline = pd.read_csv('./baseline.csv')\n",
    "baseline['target'].value_counts()\n",
    "\n",
    "# f1 score로 train에서 feature 를 가지고 학습한 모델을 test 모델에 적용해서\n",
    "# 유의미하면 ?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# tree 기반 이므로 얼마나 트리 분할과 밀접한 관련이 있는 지를 본다\n",
    "\n",
    "\n",
    "# feature 하나하나 마다 shuffle하여 성능 변화 지켜보기 중요한 역할의 feature 라면 모델 서능 떨어질 것\n",
    "# weight가 양수인갑들은 중요한 값 모델에 큰 영향을 끼친다\n",
    "# contents attribute d가 중요한 featrue\n",
    "n_features = len(x_train.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%import time\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### 시각화 코드 ###\n",
    "model.feature_importances_\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.barh(np.arange(n_features), sorted(model.feature_importances_), align='center')\n",
    "plt.yticks(np.arange(n_features) , x_train.columns)\n",
    "plt.xlabel('random forest feature importance')\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 사용자 번호와 컨텐츠 번호는 관련이 없을 듯 한데 제거\n",
    "\n",
    "# 신경망에 리스트를 주입할 수 없으니 텐서로 변환\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# feature importance\n",
    "feature_names= test.columns\n",
    "forest_importances = pd.Series(importances, index=feature_names )\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title('Feature importances using MDI')\n",
    "ax.set_ylabel(\"Mena decrease in impurity\")\n",
    "fig.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# feature importance 가 높은 값에 가중치를 줘서 더 높은 정확도 필요 어떤 콘텐츠 열람하고 시청을 했느냐가 타겟\n",
    "train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#회원 선호속성과 컨텐츠 속성과의 연관관계"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 같은 사용자\n",
    "# 데이터 시각화 부터 하자\n",
    "train = train.astype(np.int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test = test.astype(np.int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 각각 feature 간의 상관관계\n",
    "# 대분류중분류소분류가 그래도 타겟과의 상관관계가 그나마 높은 것이 보인다\n",
    "import seaborn as sns\n",
    "train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.heatmap(train.corr())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 변수 나누기\n",
    "\n",
    "#\n",
    "train['d_l_match_yn']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "meta_data = []\n",
    "for col_name in train.columns:\n",
    "    if 'yn' in col_name:\n",
    "        level = 'binary'\n",
    "    elif 'attribute' in col_name:\n",
    "        level = 'nominal'\n",
    "    elif 'prefer' in col_name:\n",
    "        level = 'nominal'\n",
    "\n",
    "    f_dict = {\n",
    "        'feature_name' : col_name,\n",
    "        'level' : level\n",
    "    }\n",
    "\n",
    "    meta_data.append(f_dict)\n",
    "\n",
    "meta_df = pd.DataFrame(meta_data, columns=['feature_name', 'level'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "              feature_name    level\n0             d_l_match_yn   binary\n1             d_m_match_yn   binary\n2             d_s_match_yn   binary\n3             h_l_match_yn   binary\n4             h_m_match_yn   binary\n5             h_s_match_yn   binary\n6       person_attribute_a  nominal\n7     person_attribute_a_1  nominal\n8       person_attribute_b  nominal\n9          person_prefer_c  nominal\n10       person_prefer_d_1  nominal\n11       person_prefer_d_2  nominal\n12       person_prefer_d_3  nominal\n13         person_prefer_e  nominal\n14       person_prefer_h_1  nominal\n15       person_prefer_h_2  nominal\n16       person_prefer_h_3  nominal\n17    contents_attribute_i  nominal\n18    contents_attribute_a  nominal\n19  contents_attribute_j_1  nominal\n20    contents_attribute_j  nominal\n21    contents_attribute_c  nominal\n22    contents_attribute_k  nominal\n23    contents_attribute_l  nominal\n24    contents_attribute_d  nominal\n25    contents_attribute_m  nominal\n26    contents_attribute_e  nominal\n27    contents_attribute_h  nominal\n28                  target  nominal",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_name</th>\n      <th>level</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>d_l_match_yn</td>\n      <td>binary</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>d_m_match_yn</td>\n      <td>binary</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>d_s_match_yn</td>\n      <td>binary</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>h_l_match_yn</td>\n      <td>binary</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>h_m_match_yn</td>\n      <td>binary</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>h_s_match_yn</td>\n      <td>binary</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>person_attribute_a</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>person_attribute_a_1</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>person_attribute_b</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>person_prefer_c</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>person_prefer_d_1</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>person_prefer_d_2</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>person_prefer_d_3</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>person_prefer_e</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>person_prefer_h_1</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>person_prefer_h_2</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>person_prefer_h_3</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>contents_attribute_i</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>contents_attribute_a</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>contents_attribute_j_1</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>contents_attribute_j</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>contents_attribute_c</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>contents_attribute_k</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>contents_attribute_l</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>contents_attribute_d</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>contents_attribute_m</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>contents_attribute_e</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>contents_attribute_h</td>\n      <td>nominal</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>target</td>\n      <td>nominal</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 둘다 오버샘플링 할 필요는 없다 적절한 분포\n",
    "train['target'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vars_with_missing =  []\n",
    "\n",
    "for f in train.columns:\n",
    "    missings = train[train[f] ==-1][f].count()\n",
    "    if missings > 0:\n",
    "        vars_with_missing.append(f)\n",
    "        missings_perc = missings/train.shape[0]\n",
    "\n",
    "        print('Variable {} has {} records : ({:.2f}) with missing values'.format(f , missings , missings_perc))\n",
    "print('In total , there are {} variables with missing values'.format(len(vars_with_missing)))\n",
    "\n",
    "\n",
    "# check cardinality"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "v = meta_df[(meta_df.level == 'nominal')].index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    dist_values = train[col].value_counts().shape[0]\n",
    "    print('Variable {} has {} distinct values'.format(col, dist_values))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 변수 시각화\n",
    "\n",
    "train.dtypes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# target 이 1인 categorical value 에 대한 percentage\n",
    "\n",
    "for col in train.columns:\n",
    "    plt.figure()\n",
    "    fig , ax = plt.subplots(figsize=(20,10))\n",
    "    # Calculate the percnetage of target=1 per category value\n",
    "    cat_perc = train[[col,'target']].groupby([col], as_index=False).mean()\n",
    "    cat_perc.sort_values(by='target', ascending=False, inplace=True)\n",
    "    # Bar plot\n",
    "    # Order the bars decending on target mean\n",
    "    sns.barplot(ax=ax, x= col, y='target', data=cat_perc, order=cat_perc[col])\n",
    "    plt.ylabel(\"target percentage\", fontsize=18)\n",
    "    plt.xlabel(col,fontsize=18)\n",
    "    plt.tick_params(axis='both', which='major' , labelsize=10)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "col = meta_df[meta_df.level == 'nominal']['feature_name'].values.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "train = train.astype(np.int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dummification 하면 값의 수 만큼 컬럼이 늘어난다\n",
    "\n",
    "\n",
    "\n",
    "pd.get_dummies(train , columns = ['person_attribute_a_1'], drop_first=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PolynomialFeature\n",
    "# 각 특서으이 제곱 혹은 그 이상을 추가\n",
    "print('Before dummification we have {} variables in train'.format(train.shape[1]))\n",
    "train = pd.get_dummies(train, columns=col, drop_first=True)\n",
    "print('After dummification we have {} variables in train'.format(train.shape[1]))\n",
    "# 분산이 너무 낮으면 제거한다\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "selector = VarianceThreshold(threshold=.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 이진 변수 이고 nomial변수인데 분산이 필요한가?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "selector.fit(train.drop(['target','person_rn','contents_rn']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_select = train.select_dtypes(include=['int'])\n",
    "\n",
    "#%\n",
    "\n",
    "# barplot 이 훨씬 시각적으로 와닿는다\n",
    "\n",
    "fig1 = go.Figure(data=[trace2])\n",
    "fig1['layout'].update(layout)\n",
    "py.iplot(fig1, filename='plots')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from IPython.display import Image as PImage\n",
    "from subprocess import check_call\n",
    "from PIL import Image , ImageDraw , ImageFont\n",
    "import re\n",
    "\n",
    "\n",
    "import sklearn.metrics as mt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# accuracy = TP + TN / TP + TN + FP + FN (전체)\n",
    "accuracy = mt.accuracy_score(y_valid,y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 어느 피쳐가 들어갈 때 성능이 많이 떨어질까?\n",
    "# 모든 피쳐 조합 ?\n",
    "\n",
    "# 여부 feature 로만 모델 테스트 해보기\n",
    "\n",
    "\n",
    "cols = train.columns.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "yn_col = []\n",
    "for col in cols:\n",
    "    if 'match_yn' in col:\n",
    "        yn_col.append(col)\n",
    "\n",
    "\n",
    "\n",
    "#split마다 고려되는 features의 수 float이면 int(max_features * n_features)값이다\n",
    "\n",
    "\n",
    "yn_train = train.loc[:,yn_col].astype(np.int)\n",
    "\n",
    "(y_valid == gb.predict(x_valid)).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "attribute_col = []\n",
    "for col in cols:\n",
    "    if 'attribute' in col:\n",
    "        attribute_col.append(col)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#\n",
    "attribute_train = train.loc[:,attribute_col].astype(np.int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prefer_col = []\n",
    "for col in cols:\n",
    "    if 'prefer' in col:\n",
    "        prefer_col.append(col)\n",
    "\n",
    "prefer_train = train.loc[:, prefer_col].astype(np.int)\n",
    "\n",
    "# 여부, 선호 , 속성 각각을 트레이닝 해보고 조합도 트레이닝 해본다 ? 별로 좋은 것 같진 않지만\n",
    "\n",
    "# 선호는 확실히 모델 성능이 떨어지기는 한다\n",
    "\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(prefer_train,target,stratify=target)\n",
    "\n",
    "\n",
    "\n",
    "# 0.569면 비슷하다 그냥 이것도"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### labelencoder\n",
    "for idx, col in enumerate(train.columns):\n",
    "    if 'match' not in col and col != 'target':\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        le.fit(train_df[col].values)\n",
    "        le_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "        train[col] = train[col].apply(lambda x: le_dict.get(x,len(le_dict)))\n",
    "        val[col] = val[col].apply(lambda x: le_dict.get(x, len(le_dict)))\n",
    "        test[col] = test[col].apply(lambda x: le_dict.get(x, len(le_dict)))\n",
    "        cat_idxs.append(idx)\n",
    "        cat_dims.append(len(le_dict)+1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = train.drop('target',axis=1).values\n",
    "y_train = train['target'].values\n",
    "X_val = val.drop('target', axis=1).values\n",
    "y_val = val['target'].values\n",
    "X_test = test.values\n",
    "eval_set = (X_val, y_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cat_idxs = []\n",
    "\n",
    "cat_dims = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df['person_prefer_f']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train.drop(['person_prefer_f','person_prefer_g'],axis=1,inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_temp = pd.read_csv('./JobCare_data/train.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train['contents_open_dt'] = train_temp['contents_open_dt']\n",
    "val = train[train['contents_open_dt'].apply(lambda x: pd.Timestamp(x).month)<11 ].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "attr_a_1_mean_encode = train.groupby('person_attribute_a_1')[\"target\"].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "if 'attribute' in col:\n",
    "    name = 'person_attribute_a_1'['person_attribute_a_1'.index('attribute') + len('attribute')+1:]\n",
    "elif 'prefer' in col:\n",
    "    name ="
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nominal_cols[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nominal_cols = ['person_attribute_a_1','person_attribute_b','person_prefer_e','contents_attribute_e']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 오버피팅이 자주 발생하는 mean encoding 이므로 cross validation 과 정규화 같이 사용한다=\n",
    "# 변환 하고자 하는 범주형 변수 선택\n",
    "# 범주형 변수 그룹화 -> 타깃 변수 총합 합계\n",
    "# 범주형 변수 그룹화 타깃 빈도수 합계\n",
    "# 총합을 카운트로 나누고 본래 범주 값에 업데이트\n",
    "# 여러가지 방법으로 적용 가능하다\n",
    "# 비슷한 범주 사이에 있는 관계 표현 특징, 범주와 타깃사이에만 국한된다\n",
    "# 범주가 많은 경우 이 방법은 데이터를 훨씬 더 단순화 한다\n",
    "\n",
    "for col in nominal_cols:\n",
    "    if 'attribute' in col:\n",
    "        name = col[col.index('attribute') + len('attribute')+1:]\n",
    "        var_name = 'attr_{}_mean_encode'.format(name)\n",
    "        locals()[var_name] = train.groupby(col)[\"target\"].mean()\n",
    "    elif 'prefer' in col:\n",
    "        name = col[col.index('prefer') + len('prefer')+1:]\n",
    "        var_name = 'prefer_{}_mean_encode'.format(name)\n",
    "        locals()[var_name] = train.groupby(col)[\"target\"].mean()\n",
    "\n",
    "    train.loc[:,var_name] = train[col].map(locals()[var_name])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'target'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/c6/2j22b1rd02778st12c0mcr3w0000gn/T/ipykernel_828/3600693000.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtarget\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtarget\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mtrain\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'target'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mx_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx_valid\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_valid\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtarget\u001B[0m \u001B[0;34m,\u001B[0m \u001B[0mtest_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.3\u001B[0m \u001B[0;34m,\u001B[0m \u001B[0mshuffle\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstratify\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrandom_state\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m34\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mrf\u001B[0m\u001B[0;34m=\u001B[0m \u001B[0mRandomForestClassifier\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn_estimators\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m150\u001B[0m \u001B[0;34m,\u001B[0m \u001B[0mmax_depth\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m8\u001B[0m \u001B[0;34m,\u001B[0m \u001B[0mmin_samples_split\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_features\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_jobs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m \u001B[0;34m,\u001B[0m\u001B[0mrandom_state\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36m__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   5463\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_info_axis\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_can_hold_identifiers_and_holds_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5464\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 5465\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mobject\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__getattribute__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   5466\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5467\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__setattr__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'target'"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target = train.target\n",
    "train = train.drop('target',axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "train = train.drop(['contents_open_dt','id'],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "data": {
      "text/plain": "RandomForestClassifier(max_depth=8, max_features=0.2, min_samples_split=4,\n                       n_estimators=150, n_jobs=-1, random_state=0)"
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(train,target , test_size=0.3 , shuffle=True, stratify=target, random_state=34)\n",
    "\n",
    "rf= RandomForestClassifier(n_estimators=150 , max_depth=8 , min_samples_split=4, max_features=0.2, n_jobs=-1 ,random_state=0)\n",
    "\n",
    "rf.fit(x_train,y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5970076899578978"
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 과적합일진 모르지만 그래도많이올랐따 만족할 수 없음\n",
    "# 0.57정도에서 0.59 정도로 mean encoding 으로 상승\n",
    "\n",
    "(y_valid == rf.predict(x_valid)).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 위와 다르게 smooth한 평균을 계산하고 적용하는 방법\n",
    "# 1. 평균을 계산\n",
    "Mean = train['target'].mean()\n",
    "weight = 100\n",
    "for col in nominal_cols:\n",
    "\n",
    "    if 'attribute' in col:\n",
    "        name = col[col.index('attribute') + len('attribute')+1:]\n",
    "        var_name = 'attr_{}_mean_encode'.format(name)\n",
    "\n",
    "    elif 'prefer' in col:\n",
    "        name = col[col.index('prefer') + len('prefer')+1:]\n",
    "        var_name = 'prefer_{}_mean_encode'.format(name)\n",
    "\n",
    "    # 2. 각 그룹에 대한 값들의 빈도와 평균을 계산\n",
    "    Agg = train.groupby(col)['target'].agg(['count','mean'])\n",
    "    counts = Agg['count']\n",
    "    #%%\n",
    "    means = Agg['mean']\n",
    "\n",
    "    # 3. 'smooth'한 평균을 계산\n",
    "    smooth = (counts * means + weight * means) / (counts+weight)\n",
    "    print(smooth)\n",
    "\n",
    "    # smooth한 평균에 따라 각 값을 대체하는 것\n",
    "    train.loc[:,'smooth_'+var_name] = train[col].map(smooth)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train=train.drop(['smoothattr_a_1_mean_encode',\t'smoothattr_b_mean_encode'\t,'smoothprefer_e_mean_encode'\t,'smoothattr_e_mean_encode'],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train  =train.drop(['attr_a_1_mean_encode','attr_b_mean_encode','prefer_e_mean_encode','attr_e_mean_encode','target'],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(train,target , test_size=0.3 , shuffle=True, stratify=target, random_state=34)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/c6/2j22b1rd02778st12c0mcr3w0000gn/T/ipykernel_828/2823097199.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mrf\u001B[0m\u001B[0;34m=\u001B[0m \u001B[0mRandomForestClassifier\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn_estimators\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m150\u001B[0m \u001B[0;34m,\u001B[0m \u001B[0mmax_depth\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m8\u001B[0m \u001B[0;34m,\u001B[0m \u001B[0mmin_samples_split\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_features\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_jobs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m \u001B[0;34m,\u001B[0m\u001B[0mrandom_state\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mrf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_train\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0my_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;31m# 과적합일진 모르지만 그래도많이올랐따 만족할 수 없음\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0my_valid\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mrf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_valid\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "rf= RandomForestClassifier(n_estimators=150 , max_depth=8 , min_samples_split=4, max_features=0.2, n_jobs=-1 ,random_state=0)\n",
    "\n",
    "rf.fit(x_train,y_train)\n",
    "# 과적합일진 모르지만 그래도많이올랐따 만족할 수 없음\n",
    "(y_valid == rf.predict(x_valid)).mean()\n",
    "train = train.drop(['id','contents_open_dt','person_rn','contents_rn'],axis=1)\n",
    "target = train.target\n",
    "train = train.drop('target',axis=1)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train,target , test_size=0.3 , shuffle=True, stratify=y, random_state=34)\n",
    "rf= RandomForestClassifier(n_estimators=150 , max_depth=8 , min_samples_split=4, max_features=0.2, n_jobs=-1 ,random_state=0)\n",
    "rf.fit(x_train,y_train)\n",
    "\n",
    "# 과적합일진 모르지만 그래도많이올랐따 만족할 수 없음 정확도 향상\n",
    "(y_valid == rf.predict(x_valid)).mean()\n",
    "####### Weight of Evidence Encoding (WoE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#%#%\n",
    "# 각 범주가 target = 1일 확률 (좋은(good) = 1 일 확률) 을 계산한다.\n",
    "\n",
    "train = train.drop(['person_rn'],axis=1)\n",
    "cols.pop('person_rn')\n",
    "cols.remove('person_rn')\n",
    "cols.remove('contents_rn')\n",
    "cols.remove('target')\n",
    "#### WoE Encoding #######"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "['d_l_match_yn',\n 'd_m_match_yn',\n 'd_s_match_yn',\n 'h_l_match_yn',\n 'h_m_match_yn',\n 'h_s_match_yn',\n 'person_attribute_a',\n 'person_attribute_a_1',\n 'person_attribute_b',\n 'person_prefer_c',\n 'person_prefer_d_1',\n 'person_prefer_d_2',\n 'person_prefer_d_3',\n 'person_prefer_e',\n 'person_prefer_f',\n 'person_prefer_g',\n 'person_prefer_h_1',\n 'person_prefer_h_2',\n 'person_prefer_h_3',\n 'contents_attribute_i',\n 'contents_attribute_a',\n 'contents_attribute_j_1',\n 'contents_attribute_j',\n 'contents_attribute_c',\n 'contents_attribute_k',\n 'contents_attribute_l',\n 'contents_attribute_d',\n 'contents_attribute_m',\n 'contents_attribute_e',\n 'contents_attribute_h',\n 'person_rn',\n 'contents_rn',\n 'target']"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['d_l_match_yn', 'd_m_match_yn', 'd_s_match_yn', 'h_l_match_yn',\n       'h_m_match_yn', 'h_s_match_yn', 'person_attribute_a',\n       'person_attribute_a_1', 'person_attribute_b', 'person_prefer_c',\n       'person_prefer_d_1', 'person_prefer_d_2', 'person_prefer_d_3',\n       'person_prefer_e', 'person_prefer_f', 'person_prefer_g',\n       'person_prefer_h_1', 'person_prefer_h_2', 'person_prefer_h_3',\n       'contents_attribute_i', 'contents_attribute_a',\n       'contents_attribute_j_1', 'contents_attribute_j',\n       'contents_attribute_c', 'contents_attribute_k', 'contents_attribute_l',\n       'contents_attribute_d', 'contents_attribute_m', 'contents_attribute_e',\n       'contents_attribute_h', 'contents_open_dt', 'target'],\n      dtype='object')"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "def woe_encoding(df):\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            woe_df = df.groupby(col)['target'].mean()\n",
    "        # if col in test.columns:\n",
    "        #     test['{}_woe_encode'.format(col)] = test[col].map(woe_df)\n",
    "        woe_df = pd.DataFrame(woe_df)\n",
    "        woe_df = woe_df.rename(columns = {'target':'good'})\n",
    "        woe_df['bad'] = 1-woe_df.good\n",
    "        woe_df['bad'] = np.where(woe_df['bad'] == 0 , 1e-6,woe_df['bad'])\n",
    "        woe_df['WoE'] = np.log(woe_df.good/woe_df.bad)\n",
    "        if col in df.columns:\n",
    "            df.loc[:,'{}_woe_encode'.format(col)] = df[col].map(woe_df['WoE'])\n",
    "            # test데이터에는 train 데이터 트레이닝 시 같이 매핑해준다 . target값이 없으므로\n",
    "            test.loc[:,'{}_woe_encode',format(col)] = test[col].map(woe_df['WoE'])\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Column not found: target'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/c6/2j22b1rd02778st12c0mcr3w0000gn/T/ipykernel_828/3702007030.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mwoe_encoding\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/var/folders/c6/2j22b1rd02778st12c0mcr3w0000gn/T/ipykernel_828/343846711.py\u001B[0m in \u001B[0;36mwoe_encoding\u001B[0;34m(df)\u001B[0m\n\u001B[1;32m      2\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mcol\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcols\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcol\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m             \u001B[0mwoe_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroupby\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'target'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m         \u001B[0;31m# if col in test.columns:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m         \u001B[0;31m#     test['{}_woe_encode'.format(col)] = test[col].map(woe_df)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/pandas/core/groupby/generic.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   1540\u001B[0m                 \u001B[0mstacklevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1541\u001B[0m             )\n\u001B[0;32m-> 1542\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__getitem__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1543\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1544\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_gotitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mndim\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msubset\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/pandas/core/base.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    278\u001B[0m             \u001B[0;31m# error: \"SelectionMixin\" has no attribute \"obj\"  [attr-defined]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    279\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mkey\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# type: ignore[attr-defined]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 280\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"Column not found: {key}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    281\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_gotitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mndim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    282\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'Column not found: target'"
     ]
    }
   ],
   "source": [
    "test.loc[:,'{}_woe_encode',format(col)] = test[col].map(woe_df['WoE'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "data": {
      "text/plain": "{'person_prefer_f_woe_encode',\n 'person_prefer_g_woe_encode',\n 'target',\n 'target_woe_encode'}"
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train.columns) - set(test.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'person_prefer_f'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001B[0m in \u001B[0;36mget_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3079\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3080\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3081\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'person_prefer_f'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/c6/2j22b1rd02778st12c0mcr3w0000gn/T/ipykernel_828/2278284377.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtest\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'person_prefer_f'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/pandas/core/frame.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3022\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnlevels\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3023\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_getitem_multilevel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3024\u001B[0;31m             \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3025\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mis_integer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindexer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3026\u001B[0m                 \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mindexer\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001B[0m in \u001B[0;36mget_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3080\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3081\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3082\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3083\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3084\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtolerance\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'person_prefer_f'"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train['ta\"rget']= target\n",
    "woe_df = train.groupby('person_attribute_a_1')['target'].mean()\n",
    "woe_df = pd.DataFrame(woe_df)\n",
    "\n",
    "# 칼럼의 이름을 \"good\" 으로 바꾸어 좀 더 이해하기 쉽게 한다\n",
    "woe_df = woe_df.rename(columns = {'target':'good'})\n",
    "\n",
    "woe_df['bad'] = 1 -woe_df.good\n",
    "#분모에 최소한의 값을 더하여 0으로 나뉘는 일을 막는다\n",
    "woe_df['bad'] = np.where(woe_df['bad']  == 0, 1e-6,woe_df['bad'])\n",
    "#WoE를 계산한다\n",
    "woe_df['WoE'] = np.log(woe_df.good/woe_df.bad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 가중치 인코딩이니 좀 더 내가 원했듯이 별 차이 없는 (0이어도 타겟값이 1:1비율로 퍼져있는)\n",
    "# 그런 변수는 woe값이 아주 낮게 나오는게 아주맘에든다 바로이거다 전체구조를 파악하고 기억할 생각을 안해서 그렇지\n",
    "# 하나씩 기억을 하고 이루다 개발하면서 로그시스템도 척척 restfulapi 스프링 서비스 구조를 알다보니\n",
    "# 분명히 이건 이걸껀데 하면서 하나의 웹사이트에서 본 지식이 이해가 안가면 다른 웹사이트에서 본 지식과 내가 알고 있는것을 합쳐서\n",
    "# 보니 이해할 수 있었다는 것을 얻었다는게 아주중요하다 오늘 WoE 값이 내가 원하던 값\n",
    "woe_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "locals()['attr_a_1_mean_encode']\n",
    "locals()['attr_b_mean_encode']\n",
    "locals()['prefer_e_mean_encode']\n",
    "locals()['attr_e_mean_encode']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "attr_a_1_mean_encode"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 타깃에 대한 빈도수 합계\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 각 범주에 대해 , 타깃 = 1 인 확률 (좋은(Good) = 1 일 확률을 찾는다)을 찾는다\n",
    "\n",
    "\n",
    "#### PR Encoding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pr_df = train.groupby(\"person_attribute_a_1\")['target'].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pr_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pr_df = pd.DataFrame(pr_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pr_df = pr_df.rename(columns = {'target' : 'good'})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pr_df['bad'] = 1 - pr_df.good\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pr_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 분모에 최소한의 값을 더하여 0으로 나뉘는 일을 막는다\n",
    "\n",
    "pr_df['bad'] = np.where(pr_df['bad'] == 0 , 1e-6 ,  pr_df['bad'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#확률 비율을 계산한다\n",
    "\n",
    "pr_df['PR'] = pr_df.good/pr_df.bad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%d\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pr_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train.loc[:,'PR_Encode'] = train['person_attribute_a_1'].map(pr_df['PR'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 훈련 시에 생성된 mapping 값들을 타깃 데이터가주어지지 않는 kaggle competition에 사용함\n",
    "# 타겟 기반으로 생성된 훈련 타임의 피쳐들을 테스트 데이터에 적용할 수 있도록 . <- 설명"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 이 mean_encode 를 test data 에 활용\n",
    "\n",
    "#mean_encode"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test 데이터에 그냥 쓰는 것이 아니라 인코딩시켜서 적용한다 바로\n",
    "\n",
    "test['person_attr_a_1_encode'] =test['person_attribute_a_1'].map(mean_encode)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cols"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train.groupby('person_attribute_a_1')['target'].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in cols:\n",
    "    print(train.groupby(col)['target'].mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#1227 이 뭔지는 모르지만 많은 값이면서도 target 수치가 높다 어떻게 활용하지?\n",
    "# d1과 d3의 타겟 값이 많으면서도 높은 값을 차지하니 높게 가중치를 쳐야하는데\n",
    "train['person_prefer_d_1'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "group_data = train.groupby('person_prefer_d_1')['target'].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "group_data[group_data.index ==114]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "group_data[group_data.index ==102]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 이렇게 count 많으면서 반반인 feature는 버릴 수 없나\n",
    "# feature_seletion 을 했다고 하면 되잖아\n",
    "# 순서형에 대해서는 mean encode를 하고 이런 반반인 값들은\n",
    "# feature selection 을 통해 버렸다고\n",
    "# 정확도가 높지만 count 낮은 것은 버린다\n",
    "# 그래 전체를 기억하자 feature selection 의 개념도 잘못됬었다 그건 피쳐 자체를 선택하냐 마냐의 문제고\n",
    "# 그렇다면 인코딩을 통해 의미없는 밸류는 의미없도록 하는게 나을 지도\n",
    "group_data[group_data.index ==1227]\n",
    "train[train.person_prefer_d_2 == 4]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train['person_prefer_d_2'].value_counts()\n",
    "train.columns\n",
    "meta_df\n",
    "# d_l_match_yn WoE encoding\n",
    "d_l_match_woe_df = train.groupby(\"d_l_match_yn\")['target'].mean()\n",
    "d_l_match_woe_df = pd.DataFrame(d_l_match_woe_df)\n",
    "d_l_match_woe_df\n",
    "d_l_match_woe_df = d_l_match_woe_df.rename(columns = {'target' : 'good'})\n",
    "d_l_match_woe_df['bad'] = 1- d_l_match_woe_df.good\n",
    "d_l_match_woe_df['bad'] = np.where(d_l_match_woe_df['bad'] == 0 , 1e-6 , d_l_match_woe_df['bad'])\n",
    "d_l_match_woe_df['WoE'] = np.log(d_l_match_woe_df.good/woe_df.bad)\n",
    "\n",
    "d_l_match_woe_df\n",
    "\n",
    "# person_attribute_a_1 PR Encoding\n",
    "\n",
    "pr_df = train.groupby('person_attribute_a_1')['target'].mean()\n",
    "pr_df = pd.DataFrame(pr_df)\n",
    "pr_df = pr_df.rename(columns = {'target' : 'good'} )\n",
    "pr_df['bad'] = 1-pr_df.good\n",
    "pr_df['bad']  = np.where(pr_df['bad'] ==0 , 1e-6, pr_df['bad'])\n",
    "# 모델의 가중치가 update 될 때 이렇게 비슷한 값을 커지게 한 것의 변형은\n",
    "# 의미가 적을 것 같고 log를 씌워 음수 양수 나눈 것은 좀 더 달라지기는 했으므로 의미가 있으려나 오히려 더 극단적으로 될 수도\n",
    "#\n",
    "pr_df['PR'] = pr_df.good/pr_df.bad\n",
    "pr_df\n",
    "\n",
    "##### all WoE Encoding ######\n",
    "for col in cols:\n",
    "    if col in train.columns:\n",
    "        woe_df = train.groupby(col)['target'].mean()\n",
    "    if col in test.columns:\n",
    "        test['{}_woe_encode'.format(col)] = test[col].map(woe_df)\n",
    "    woe_df = pd.DataFrame(woe_df)\n",
    "    woe_df = woe_df.rename(columns = {'target':'good'})\n",
    "    woe_df['bad'] = 1-woe_df.good\n",
    "    woe_df['bad'] = np.where(woe_df['bad'] == 0 , 1e-6,woe_df['bad'])\n",
    "    woe_df['WoE'] = np.log(woe_df.good/woe_df.bad)\n",
    "    if col in train.columns:\n",
    "        train.loc[:,'{}_woe_encode'.format(col)] = train[col].map(woe_df['WoE'])\n",
    "train['target']= target\n",
    "woe_df = train.groupby('person_attribute_a_1')['target'].mean()\n",
    "woe_df = pd.DataFrame(woe_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 칼럼의 이름을 \"good\" 으로 바꾸어 좀 더 이해하기 쉽게 한다\n",
    "woe_df = woe_df.rename(columns = {'target':'good'})\n",
    "woe_df['bad'] = 1 -woe_df.good\n",
    "#분모에 최소한의 값을 더하여 0으로 나뉘는 일을 막는다\n",
    "woe_df['bad'] = np.where(woe_df['bad']  == 0, 1e-6,woe_df['bad'])\n",
    "#WoE를 계산한다\n",
    "woe_df['WoE'] = np.log(woe_df.good/woe_df.bad)\n",
    "# 가중치 인코딩이니 좀 더 내가 원했듯이 별 차이 없는 (0이어도 타겟값이 1:1비율로 퍼져있는)\n",
    "# 그런 변수는 woe값이 아주 낮게 나오는게 아주맘에든다 바로이거다 전체구조를 파악하고 기억할 생각을 안해서 그렇지\n",
    "# 하나씩 기억을 하고 이루다 개발하면서 로그시스템도 척척 restfulapi 스프링 서비스 구조를 알다보니\n",
    "# 분명히 이건 이걸껀데 하면서 하나의 웹사이트에서 본 지식이 이해가 안가면 다른 웹사이트에서 본 지식과 내가 알고 있는것을 합쳐서\n",
    "# 보니 이해할 수 있었다는 것을 얻었다는게 아주중요하다 오늘 WoE 값이 내가 원하던 값\n",
    "woe_df\n",
    "\n",
    "locals()['attr_a_1_mean_encode']\n",
    "locals()['attr_b_mean_encode']\n",
    "locals()['prefer_e_mean_encode']\n",
    "locals()['attr_e_mean_encode']\n",
    "train\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "attr_a_1_mean_encode\n",
    "# 각 범주에 대해 , 타깃 = 1 인 확률 (좋은(Good) = 1 일 확률을 찾는다)을 찾는다\n",
    "pr_df = train.groupby(\"person_attribute_a_1\")['target'].mean()\n",
    "pr_df\n",
    "pr_df = pd.DataFrame(pr_df)\n",
    "pr_df = pr_df.rename(columns = {'target' : 'good'})\n",
    "pr_df['bad'] = 1 - pr_df.good\n",
    "pr_df\n",
    "# 분모에 최소한의 값을 더하여 0으로 나뉘는 일을 막는다\n",
    "pr_df['bad'] = np.where(pr_df['bad'] == 0 , 1e-6 ,  pr_df['bad'])\n",
    "#확률 비율을 계산한다\n",
    "pr_df['PR'] = pr_df.good/pr_df.bad\n",
    "pr_df\n",
    "train.loc[:,'PR_Encode'] = train['person_attribute_a_1'].map(pr_df['PR'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 타깃에 대한 빈도수 합계\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# 훈련 시에 생성된 mapping 값들을 타깃 데이터가주어지지 않는 kaggle competition에 사용함\n",
    "# 타겟 기반으로 생성된 훈련 타임의 피쳐들을 테스트 데이터에 적용할 수 있도록 . <- 설명"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 이 mean_encode 를 test data 에 활용\n",
    "\n",
    "#mean_encode"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test 데이터에 그냥 쓰는 것이 아니라 인코딩시켜서 적용한다 바로\n",
    "\n",
    "\n",
    "for col in cols:\n",
    "    print(train.groupby(col)['target'].mean())\n",
    "\n",
    "#1227 이 뭔지는 모르지만 많은 값이면서도 target 수치가 높다 어떻게 활용하지?\n",
    "# d1과 d3의 타겟 값이 많으면서도 높은 값을 차지하니 높게 가중치를 쳐야하는데\n",
    "train['person_prefer_d_1'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "group_data = train.groupby('person_prefer_d_1')['target'].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 이렇게 count 많으면서 반반인 feature는 버릴 수 없나\n",
    "# feature_seletion 을 했다고 하면 되잖아\n",
    "# 순서형에 대해서는 mean encode를 하고 이런 반반인 값들은\n",
    "# feature selection 을 통해 버렸다고\n",
    "# 정확도가 높지만 count 낮은 것은 버린다\n",
    "# 그래 전체를 기억하자 feature selection 의 개념도 잘못됬었다 그건 피쳐 자체를 선택하냐 마냐의 문제고\n",
    "# 그렇다면 인코딩을 통해 의미없는 밸류는 의미없도록 하는게 나을 지도\n",
    "group_data[group_data.index ==1227]\n",
    "\n",
    "train[train.person_prefer_d_2 == 4]\n",
    "train['person_prefer_d_2'].value_counts()\n",
    "\n",
    "\n",
    "train.columns\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meta_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d_l_match_woe_df = train.groupby(\"d_l_match_yn\")['target'].mean()\n",
    "d_l_match_woe_df = pd.DataFrame(d_l_match_woe_df)\n",
    "\n",
    "d_l_match_woe_df\n",
    "d_l_match_woe_df = d_l_match_woe_df.rename(columns = {'target' : 'good'})\n",
    "d_l_match_woe_df['bad'] = 1- d_l_match_woe_df.good"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd_l_match_woe_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/c6/2j22b1rd02778st12c0mcr3w0000gn/T/ipykernel_828/3327872933.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0md_l_match_woe_df\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'bad'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwhere\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0md_l_match_woe_df\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'bad'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m \u001B[0;34m,\u001B[0m \u001B[0;36m1e-6\u001B[0m \u001B[0;34m,\u001B[0m \u001B[0md_l_match_woe_df\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'bad'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0md_l_match_woe_df\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'WoE'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlog\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0md_l_match_woe_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgood\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mwoe_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbad\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroupby\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"d_l_match_yn\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'target'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0md_l_match_woe_df\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mpr_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroupby\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'person_attribute_a_1'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'target'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'd_l_match_woe_df' is not defined"
     ]
    }
   ],
   "source": [
    "d_l_match_woe_df['bad'] = np.where(d_l_match_woe_df['bad'] == 0 , 1e-6 , d_l_match_woe_df['bad'])\n",
    "d_l_match_woe_df['WoE'] = np.log(d_l_match_woe_df.good/woe_df.bad)\n",
    "train.groupby(\"d_l_match_yn\")['target'].mean()\n",
    "d_l_match_woe_df\n",
    "pr_df = train.groupby('person_attribute_a_1')['target'].mean()\n",
    "pr_df = pd.DataFrame(pr_df)\n",
    "pr_df = pr_df.rename(columns = {'target' : 'good'} )\n",
    "pr_df['bad'] = 1-pr_df.good\n",
    "pr_df['bad']  = np.where(pr_df['bad'] ==0 , 1e-6, pr_df['bad'])\n",
    "# 모델의 가중치가 update 될 때 이렇게 비슷한 값을 커지게 한 것의 변형은\n",
    "# 의미가 적을 것 같고 log를 씌워 음수 양수 나눈 것은 좀 더 달라지기는 했으므로 의미가 있으려나 오히려 더 극단적으로 될 수도\n",
    "#\n",
    "pr_df['PR'] = pr_df.good/pr_df.bad\n",
    "pr_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}